[
  {
    "objectID": "notebooks/04-astro-case-study-clustering-train.html",
    "href": "notebooks/04-astro-case-study-clustering-train.html",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Fit Clustering Pipelines on Training Data\n## this code chunk fits the clustering pipelines on the training data\n\n#### choose imputation methods ####\ndata_ls &lt;- list(\n  \"Mean-imputed\" = data_mean_imputed$train,\n  \"RF-imputed\" = data_rf_imputed$train\n)\n\n#### choose number of features ####\nfeature_modes &lt;- list(\n  \"Small\" = 7,\n  \"Medium\" = 11,\n  \"Big\" = 19\n)\n\n#### choose ks ####\nks &lt;- 2:30\n\n#### choose dimension reduction methods ####\n# raw data\nidentity_fun_ls &lt;- list(\"Raw\" = function(x) x)\n\n# pca\npca_fun_ls &lt;- list(\"PCA\" = purrr::partial(fit_pca, ndim = 4))\n\n# tsne\ntsne_perplexities &lt;- c(30, 100)\ntsne_fun_ls &lt;- purrr::map(\n  tsne_perplexities,\n  ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n) |&gt; \n  setNames(sprintf(\"tSNE (perplexity = %d)\", tsne_perplexities))\n\n# putting it together\ndr_fun_ls &lt;- c(\n  identity_fun_ls,\n  pca_fun_ls,\n  tsne_fun_ls\n)\n\n#### choose clustering methods ####\n# kmeans\nkmeans_fun_ls &lt;- list(\"K-means\" = purrr::partial(fit_kmeans, ks = ks))\n\n# hierarchical clustering\nhclust_params &lt;- list(\n  d = c(\"euclidean\"),\n  linkage = c(\"complete\", \"ward.D\")\n) |&gt; \n  expand.grid()\nhclust_fun_ls &lt;- purrr::map(\n  1:nrow(hclust_params),\n  ~ purrr::partial(\n    fit_hclust, \n    ks = ks,\n    d = hclust_params$d[[.x]],\n    linkage = hclust_params$linkage[[.x]]\n  )\n) |&gt; \n  setNames(\n    sprintf(\n      \"Hierarchical (dist = %s, link = %s)\",\n      hclust_params$d, hclust_params$linkage\n    )\n  )\n\n# spectral clustering\nn_neighbors &lt;- c(5, 30, 60, 100)\nspectral_fun_ls &lt;- purrr::map(\n  n_neighbors,\n  ~ purrr::partial(\n    fit_spectral_clustering, \n    ks = ks,\n    affinity = \"nearest_neighbors\",\n    n_neighbors = .x\n  )\n) |&gt; \n  setNames(sprintf(\"Spectral (n_neighbors = %s)\", n_neighbors))\n\n# putting it together\nclust_fun_ls &lt;- c(\n  kmeans_fun_ls,\n  hclust_fun_ls,\n  spectral_fun_ls\n)\n\n#### Fit Clustering Pipelines ####\npipe_tib &lt;- tidyr::expand_grid(\n  data = data_ls,\n  feature_mode = feature_modes,\n  dr_method = dr_fun_ls,\n  clust_method = clust_fun_ls\n) |&gt; \n  dplyr::mutate(\n    impute_mode_name = names(data),\n    feature_mode_name = names(feature_mode),\n    dr_method_name = names(dr_method),\n    clust_method_name = names(clust_method),\n    name = stringr::str_glue(\n      \"{clust_method_name} [{impute_mode_name} + {feature_mode_name} + {dr_method_name}]\"\n    )\n  ) |&gt; \n  # remove some clustering pipelines to reduce computation burden\n  dplyr::filter(\n    # remove all big feature set + dimension-reduction runs\n    !((dr_method_name != \"Raw\") & (feature_mode_name == \"Big\"))\n  )\npipe_ls &lt;- split(pipe_tib, seq_len(nrow(pipe_tib))) |&gt; \n  setNames(pipe_tib$name)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"clustering_fits_train.rds\")\nif (!file.exists(fit_results_fname)) {\n  library(future)\n  plan(multisession, workers = NCORES)\n  \n  # fit clustering pipelines (if not already cached)\n  clust_fit_ls &lt;- furrr::future_map(\n    pipe_ls,\n    function(pipe_df) {\n      g &lt;- create_preprocessing_pipeline(\n        feature_mode = pipe_df$feature_mode[[1]],\n        preprocess_fun = pipe_df$dr_method[[1]]\n      )\n      clust_out &lt;- pipe_df$clust_method[[1]](\n        data = pipe_df$data[[1]], preprocess_fun = g\n      )\n      return(clust_out)\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        ks = ks,\n        create_preprocessing_pipeline = create_preprocessing_pipeline,\n        get_abundance_data = get_abundance_data,\n        tsne_perplexities = tsne_perplexities,\n        hclust_params = hclust_params,\n        n_neighbors = n_neighbors,\n        fit_kmeans = fit_kmeans,\n        fit_hclust = fit_hclust,\n        fit_spectral_clustering = fit_spectral_clustering\n      )\n    )\n  )\n  # save fitted clustering pipelines\n  saveRDS(clust_fit_ls, file = fit_results_fname)\n} else {\n  # read in fitted clustering pipelines (if already cached)\n  clust_fit_ls &lt;- readRDS(fit_results_fname)\n}\n\n\nTo see and interact with clustering results on training data, we have developed a shiny app. Run locally in R/RStudio:\n\nview_clusters_app()",
    "crumbs": [
      "5 Clustering (train)"
    ]
  },
  {
    "objectID": "notebooks/04-astro-case-study-clustering-train.html#fit-clustering-pipelines-on-training-data",
    "href": "notebooks/04-astro-case-study-clustering-train.html#fit-clustering-pipelines-on-training-data",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Fit Clustering Pipelines on Training Data\n## this code chunk fits the clustering pipelines on the training data\n\n#### choose imputation methods ####\ndata_ls &lt;- list(\n  \"Mean-imputed\" = data_mean_imputed$train,\n  \"RF-imputed\" = data_rf_imputed$train\n)\n\n#### choose number of features ####\nfeature_modes &lt;- list(\n  \"Small\" = 7,\n  \"Medium\" = 11,\n  \"Big\" = 19\n)\n\n#### choose ks ####\nks &lt;- 2:30\n\n#### choose dimension reduction methods ####\n# raw data\nidentity_fun_ls &lt;- list(\"Raw\" = function(x) x)\n\n# pca\npca_fun_ls &lt;- list(\"PCA\" = purrr::partial(fit_pca, ndim = 4))\n\n# tsne\ntsne_perplexities &lt;- c(30, 100)\ntsne_fun_ls &lt;- purrr::map(\n  tsne_perplexities,\n  ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n) |&gt; \n  setNames(sprintf(\"tSNE (perplexity = %d)\", tsne_perplexities))\n\n# putting it together\ndr_fun_ls &lt;- c(\n  identity_fun_ls,\n  pca_fun_ls,\n  tsne_fun_ls\n)\n\n#### choose clustering methods ####\n# kmeans\nkmeans_fun_ls &lt;- list(\"K-means\" = purrr::partial(fit_kmeans, ks = ks))\n\n# hierarchical clustering\nhclust_params &lt;- list(\n  d = c(\"euclidean\"),\n  linkage = c(\"complete\", \"ward.D\")\n) |&gt; \n  expand.grid()\nhclust_fun_ls &lt;- purrr::map(\n  1:nrow(hclust_params),\n  ~ purrr::partial(\n    fit_hclust, \n    ks = ks,\n    d = hclust_params$d[[.x]],\n    linkage = hclust_params$linkage[[.x]]\n  )\n) |&gt; \n  setNames(\n    sprintf(\n      \"Hierarchical (dist = %s, link = %s)\",\n      hclust_params$d, hclust_params$linkage\n    )\n  )\n\n# spectral clustering\nn_neighbors &lt;- c(5, 30, 60, 100)\nspectral_fun_ls &lt;- purrr::map(\n  n_neighbors,\n  ~ purrr::partial(\n    fit_spectral_clustering, \n    ks = ks,\n    affinity = \"nearest_neighbors\",\n    n_neighbors = .x\n  )\n) |&gt; \n  setNames(sprintf(\"Spectral (n_neighbors = %s)\", n_neighbors))\n\n# putting it together\nclust_fun_ls &lt;- c(\n  kmeans_fun_ls,\n  hclust_fun_ls,\n  spectral_fun_ls\n)\n\n#### Fit Clustering Pipelines ####\npipe_tib &lt;- tidyr::expand_grid(\n  data = data_ls,\n  feature_mode = feature_modes,\n  dr_method = dr_fun_ls,\n  clust_method = clust_fun_ls\n) |&gt; \n  dplyr::mutate(\n    impute_mode_name = names(data),\n    feature_mode_name = names(feature_mode),\n    dr_method_name = names(dr_method),\n    clust_method_name = names(clust_method),\n    name = stringr::str_glue(\n      \"{clust_method_name} [{impute_mode_name} + {feature_mode_name} + {dr_method_name}]\"\n    )\n  ) |&gt; \n  # remove some clustering pipelines to reduce computation burden\n  dplyr::filter(\n    # remove all big feature set + dimension-reduction runs\n    !((dr_method_name != \"Raw\") & (feature_mode_name == \"Big\"))\n  )\npipe_ls &lt;- split(pipe_tib, seq_len(nrow(pipe_tib))) |&gt; \n  setNames(pipe_tib$name)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"clustering_fits_train.rds\")\nif (!file.exists(fit_results_fname)) {\n  library(future)\n  plan(multisession, workers = NCORES)\n  \n  # fit clustering pipelines (if not already cached)\n  clust_fit_ls &lt;- furrr::future_map(\n    pipe_ls,\n    function(pipe_df) {\n      g &lt;- create_preprocessing_pipeline(\n        feature_mode = pipe_df$feature_mode[[1]],\n        preprocess_fun = pipe_df$dr_method[[1]]\n      )\n      clust_out &lt;- pipe_df$clust_method[[1]](\n        data = pipe_df$data[[1]], preprocess_fun = g\n      )\n      return(clust_out)\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        ks = ks,\n        create_preprocessing_pipeline = create_preprocessing_pipeline,\n        get_abundance_data = get_abundance_data,\n        tsne_perplexities = tsne_perplexities,\n        hclust_params = hclust_params,\n        n_neighbors = n_neighbors,\n        fit_kmeans = fit_kmeans,\n        fit_hclust = fit_hclust,\n        fit_spectral_clustering = fit_spectral_clustering\n      )\n    )\n  )\n  # save fitted clustering pipelines\n  saveRDS(clust_fit_ls, file = fit_results_fname)\n} else {\n  # read in fitted clustering pipelines (if already cached)\n  clust_fit_ls &lt;- readRDS(fit_results_fname)\n}\n\n\nTo see and interact with clustering results on training data, we have developed a shiny app. Run locally in R/RStudio:\n\nview_clusters_app()",
    "crumbs": [
      "5 Clustering (train)"
    ]
  },
  {
    "objectID": "notebooks/04-astro-case-study-clustering-train.html#hyperparameter-tuning-and-model-selection-via-cluster-stability",
    "href": "notebooks/04-astro-case-study-clustering-train.html#hyperparameter-tuning-and-model-selection-via-cluster-stability",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "Hyperparameter Tuning and Model Selection via Cluster Stability",
    "text": "Hyperparameter Tuning and Model Selection via Cluster Stability\n\n\nShow Code to Tune/Evaluate Clustering Pipelines via Cluster Stability\n## this code chunk performs model selection and hyperparameter tuning for the trained clustering pipelines\n\n#### ModelExplorer hyperparameters ####\nn_subsamples &lt;- 50 # Number of subsamples\nsubsample_frac &lt;- 0.8 # Proportion of data to subsample\nmax_pairs &lt;- 100  # max num of pairs of subsamples to evaluate cluster stability\nmetric &lt;- \"ARI\"  # metrics to compute for cluster stability\n\nn &lt;- nrow(data_ls$`Mean-imputed`)\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"clustering_subsampled_fits.rds\")\nstability_results_fname &lt;- file.path(\n  RESULTS_PATH, sprintf(\"clustering_subsampled_stability_%s.RData\", metric)\n)\nif (!file.exists(fit_results_fname) ||\n    !file.exists(stability_results_fname)) {\n  library(future)\n  plan(multisession, workers = NCORES)\n\n  # fit clustering pipelines on subsampled data (if not already cached)\n  clust_subsampled_fit_ls &lt;- purrr::map(\n    1:n_subsamples,\n    function(b) {\n      # get subsampled indices\n      subsampled_idxs &lt;- sort(\n        sample(1:n, size = subsample_frac * n, replace = FALSE)\n      )\n      furrr::future_map(\n        pipe_ls,\n        function(pipe_df) {\n          # fit clustering pipeline on subsample\n          g &lt;- create_preprocessing_pipeline(\n            feature_mode = pipe_df$feature_mode[[1]],\n            preprocess_fun = pipe_df$dr_method[[1]]\n          )\n          clust_out &lt;- pipe_df$clust_method[[1]](\n            data = pipe_df$data[[1]][subsampled_idxs, ], preprocess_fun = g\n          )\n          # return only the cluster ids for subsample\n          cluster_ids &lt;- purrr::map(\n            clust_out$cluster_ids,\n            function(cluster_ids) {\n              out &lt;- rep(NA, nrow(pipe_df$data[[1]]))\n              out[subsampled_idxs] &lt;- cluster_ids\n              return(out)\n            }\n          )\n          return(cluster_ids)\n        },\n        .options = furrr::furrr_options(\n          seed = TRUE, \n          globals = list(\n            ks = ks,\n            subsampled_idxs = subsampled_idxs,\n            create_preprocessing_pipeline = create_preprocessing_pipeline,\n            get_abundance_data = get_abundance_data,\n            tsne_perplexities = tsne_perplexities,\n            hclust_params = hclust_params,\n            n_neighbors = n_neighbors,\n            fit_kmeans = fit_kmeans,\n            fit_hclust = fit_hclust,\n            fit_spectral_clustering = fit_spectral_clustering\n          )\n        )\n      )\n    }\n  ) |&gt; \n    setNames(as.character(1:n_subsamples)) |&gt; \n    purrr::list_flatten(name_spec = \"{inner} {outer}\") |&gt; \n    purrr::list_flatten(name_spec = \"{inner}: {outer}\")\n  # save fitted clustering pipelines on subsampled data\n  saveRDS(clust_subsampled_fit_ls, file = fit_results_fname)\n  \n  # create tibble of clustering results\n  clusters_tib &lt;- tibble::tibble(\n    name = names(clust_subsampled_fit_ls),\n    cluster_ids = clust_subsampled_fit_ls\n  ) |&gt; \n    annotate_clustering_results()\n  \n  # evaluate stability of clustering results per pipeline\n  stability_per_pipeline &lt;- clusters_tib |&gt; \n    dplyr::group_by(\n      k, pipeline_name, \n      impute_mode_name, feature_mode_name, dr_method_name, clust_method_name\n    ) |&gt;\n    dplyr::summarise(\n      cluster_list = list(cluster_ids),\n      .groups = \"drop\"\n    )\n  stability_vals &lt;- furrr::future_map(\n    stability_per_pipeline$cluster_list,\n    function(clusters) {\n      cluster_stability(\n        clusters, max_pairs = max_pairs, metrics = metric\n      )[[metric]]\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        cluster_stability = cluster_stability,\n        max_pairs = max_pairs,\n        metric = metric\n      )\n    )\n  )\n  stability_per_pipeline &lt;- stability_per_pipeline |&gt; \n    dplyr::mutate(\n      group_name = pipeline_name,\n      stability = !!stability_vals\n    )\n  \n  # evaluate stability of clustering results per clustering method\n  stability_per_clust_method &lt;- clusters_tib |&gt; \n    dplyr::group_by(\n      k, clust_method_name\n    ) |&gt;\n    dplyr::summarise(\n      pipeline_names = list(unique(pipeline_name)),\n      cluster_list = list(cluster_ids),\n      .groups = \"drop\"\n    )\n  stability_vals &lt;- furrr::future_map2(\n    stability_per_clust_method$cluster_list,\n    stability_per_clust_method$pipeline_names,\n    function(clusters, pipe_names) {\n      cluster_stability(\n        clusters, max_pairs = max_pairs * length(pipe_names), metrics = metric\n      )[[metric]]\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        cluster_stability = cluster_stability,\n        max_pairs = max_pairs,\n        metric = metric\n      )\n    )\n  )\n  stability_per_clust_method &lt;- stability_per_clust_method |&gt;\n    dplyr::mutate(\n      group_name = clust_method_name,\n      stability = !!stability_vals\n    )\n  \n  save(\n    stability_per_pipeline, \n    stability_per_clust_method, \n    file = stability_results_fname\n  )\n} else {\n  # read in fitted clustering pipelines on subsampled data (if already cached)\n  clust_subsampled_fit_ls &lt;- readRDS(fit_results_fname)\n  # read in evaluated stability of clustering results\n  load(stability_results_fname)\n}\n\n\n\nPer Clustering MethodPer Pipeline\n\n\n\n\n\n\nStability of clustering pipelines across the number of clusters, measured via the adjusted Rand Index (ARI).\n\n\n\n\n\n\n\n\nStability of clustering pipelines across the number of clusters, measured via the adjusted Rand Index (ARI).",
    "crumbs": [
      "5 Clustering (train)"
    ]
  },
  {
    "objectID": "notebooks/04-astro-case-study-clustering-train.html#estimate-consensus-clusters",
    "href": "notebooks/04-astro-case-study-clustering-train.html#estimate-consensus-clusters",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "Estimate Consensus Clusters",
    "text": "Estimate Consensus Clusters\n\n\nShow Code to Estimate Consensus Clusters\nbest_clust_methods &lt;- list(\n  \"k = 2: Spectral (n_neighbors = 60)\" = list(\n    clust_method_name = \"Spectral (n_neighbors = 60)\",\n    k = 2\n  ),\n  \"k = 9: Spectral (n_neighbors = 60)\" = list(\n    clust_method_name = \"Spectral (n_neighbors = 60)\",\n    k = 9\n  ),\n  \"k = 8: K-means\" = list(\n    clust_method_name = \"K-means\", \n    k = 8\n  )\n)\nclust_fit_ls &lt;- purrr::map(clust_fit_ls, ~ .x$cluster_ids) |&gt; \n  purrr::list_flatten(name_spec = \"{inner}: {outer}\")\n\nconsensus_clusters_results_path &lt;- file.path(\n  RESULTS_PATH, \"consensus_clusters_train.rds\"\n)\nconsensus_nbhd_results_path &lt;- file.path(\n  RESULTS_PATH, \"consensus_neighborhood_matrices_train.rds\"\n)\nif (!file.exists(consensus_clusters_results_path) | \n    !file.exists(consensus_nbhd_results_path)) {\n  nbhd_mat_ls &lt;- list()\n  consensus_out_ls &lt;- list()\n  for (i in 1:length(best_clust_methods)) {\n    clust_method_name &lt;- best_clust_methods[[i]]$clust_method_name\n    k &lt;- best_clust_methods[[i]]$k\n    key &lt;- names(best_clust_methods)[i]\n    \n    stable_clusters_tib_all &lt;- tibble::tibble(\n      name = names(clust_fit_ls),\n      cluster_ids = clust_fit_ls\n    ) |&gt; \n      annotate_clustering_results() |&gt; \n      dplyr::filter(\n        k == !!k,\n        clust_method_name == !!clust_method_name\n      )\n  \n    keep_clust_ls &lt;- stable_clusters_tib_all$cluster_ids\n    # compute neighborhood matrix\n    nbhd_mat_ls[[key]] &lt;- get_consensus_neighborhood_matrix(keep_clust_ls)\n    # aggregate stable clusters using consensus clustering\n    consensus_out_ls[[key]] &lt;- fit_consensus_clusters(nbhd_mat_ls[[key]], k = k)[\"cluster_ids\"]\n  }\n  saveRDS(consensus_out_ls, file = consensus_clusters_results_path)\n  saveRDS(nbhd_mat_ls, file = consensus_nbhd_results_path)\n} else {\n  # read in fitted clustering pipelines (if already cached)\n  consensus_out_ls &lt;- readRDS(consensus_clusters_results_path)\n  nbhd_mat_ls &lt;- readRDS(consensus_nbhd_results_path)\n}\n\n\n\nk = 2: Spectral (n_neighbors = 60)k = 8: K-means\n\n\n\nOverviewLocal StabilityAbundance per Cluster\n\n\n\nConsensus Neighborhood Heatmap\n\n\n\n\n\nConsensus neighborhood heatmap, aggregated across all runs using Spectral (n_neighbors = 60) and k = 2.\n\n\n\n\n\n\nGCs per Cluster\n\n\n\n\nGC composition for each estimated consensus cluster using Spectral (n_neighbors = 60) and k = 2.\n\n\n\n\n\n\nOn Galactic Coordinates\n\n\n\n\n\nGalactic coordinates of stars, colored by its local cluster stability.\n\n\n\n\n\n\nOn Dimension Reduction\n\n\n\n\n\nDimension reduction visualizations of stars, colored by its local cluster stability.\n\n\n\n\n\n\n\n\n\n\n\nAbundance of features per estimated consensus cluster.\n\n\n\n\n\n\n\n\nOverviewLocal StabilityAbundance per Cluster\n\n\n\nConsensus Neighborhood Heatmap\n\n\n\n\n\nConsensus neighborhood heatmap, aggregated across all runs using K-means and k = 8.\n\n\n\n\n\n\nGCs per Cluster\n\n\n\n\nGC composition for each estimated consensus cluster using K-means and k = 8.\n\n\n\n\n\n\nOn Galactic Coordinates\n\n\n\n\n\nGalactic coordinates of stars, colored by its local cluster stability.\n\n\n\n\n\n\nOn Dimension Reduction\n\n\n\n\n\nDimension reduction visualizations of stars, colored by its local cluster stability.\n\n\n\n\n\n\n\n\n\n\n\nAbundance of features per estimated consensus cluster.",
    "crumbs": [
      "5 Clustering (train)"
    ]
  },
  {
    "objectID": "notebooks/04-astro-case-study-clustering-train.html#aside-what-if",
    "href": "notebooks/04-astro-case-study-clustering-train.html#aside-what-if",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "Aside: What if…",
    "text": "Aside: What if…\nWhat if we had used Spectral Clustering (n neighbors = 60) with k = 9?\n\nComparison Summaryk = 9: Spectral (n_neighbors = 60)\n\n\n\nNeighborhood Heatmap\n\n\n\n\n\nConsensus neighborhood heatmap between consensus clusters using Spectral (n_neighbors = 60) with k = 9 and K-means with k = 8.\n\n\n\n\n\n\nGCs per Cluster\n\n\n\n\nComparison of GC composition for each estimated consensus cluster.\n\n\n\n\n\n\nOverviewLocal Stability\n\n\n\nConsensus Neighborhood Heatmap\n\n\n\n\n\nConsensus neighborhood heatmap, aggregated across all runs using Spectral (n_neighbors = 60) and k = 9.\n\n\n\n\n\n\nGCs per Cluster\n\n\n\n\nGC composition for each estimated consensus cluster.\n\n\n\n\n\n\nOn Galactic Coordinates\n\n\n\n\n\nGalactic coordinates of stars, colored by its local cluster stability.\n\n\n\n\n\n\nOn Dimension Reduction\n\n\n\n\n\nDimension reduction visualizations of stars, colored by its local cluster stability.\n\n\n\n\n\n\n\n\n\n\n\nWhat if we had used only the most stable clustering pipeline and done consensus clustering on the subsampled fits?\n\nComparison Summaryk = 8: K-means\n\n\n\nNeighborhood Heatmap\n\n\n\n\n\nConsensus neighborhood heatmap between consensus clusters using K-means with k = 8, aggregated across all data preprocessing pipelines versus using only the most stable clustering pipeline.\n\n\n\n\n\n\nGCs per Cluster\n\n\n\n\nComparison of GC composition for each estimated consensus cluster.\n\n\n\n\n\n\nOverviewLocal Stability\n\n\n\nConsensus Neighborhood Heatmap\n\n\n\n\n\nConsensus neighborhood heatmap, aggregated across all subsample runs using k = 8: K-means\n\n\n\n\n\n\nGCs per Cluster\n\n\n\n\nGC composition for each estimated consensus cluster.\n\n\n\n\n\n\nOn Galactic Coordinates\n\n\n\n\n\nGalactic coordinates of stars, colored by its local cluster stability.\n\n\n\n\n\n\nOn Dimension Reduction\n\n\n\n\n\nDimension reduction visualizations of stars, colored by its local cluster stability.",
    "crumbs": [
      "5 Clustering (train)"
    ]
  },
  {
    "objectID": "notebooks/02-astro-case-study-eda.html",
    "href": "notebooks/02-astro-case-study-eda.html",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code\n# get X (using mean-imputed) and metadata for EDA\nmetadata &lt;- metadata$train\nfeature_modes &lt;- list(\"small\" = 7, \"medium\" = 11, \"big\" = 19)\ntrain_data_ls &lt;- purrr::map(\n  feature_modes,\n  ~ get_abundance_data(data_mean_imputed$train, feature_mode = .x)\n)\nfeatures_ls &lt;- purrr::map(train_data_ls, ~ colnames(.x))\nX &lt;- train_data_ls$big\n\n\n\n\n\nOverallBy GC\n\n\n\nShow Code\n# plot overall distribution\nggwrappers::plot_histogram(X) +\n  ggplot2::facet_wrap(~ variable, scales = \"free_x\") +\n  ggplot2::labs(x = \"Data\")\n\n\n\n\nDistribution of abundance values per feature in (mean-imputed) training data.\n\n\n\n\n\nShow Code\n# plot boxplots per GC\nplt_df &lt;- dplyr::bind_cols(\n  X,\n  metadata |&gt; dplyr::select(GC_NAME)\n) |&gt; \n  dplyr::group_by(GC_NAME) |&gt; \n  dplyr::mutate(\n    GC_NAME = sprintf(\"%s (n = %d)\", GC_NAME, dplyr::n())\n  ) |&gt; \n  dplyr::ungroup()\nplt_vars &lt;- sort(colnames(X))\nplt_ls &lt;- list()\nfor (plt_var in plt_vars) {\n  plt_ls[[plt_var]] &lt;- plt_df |&gt; \n    ggplot2::ggplot() +\n    ggplot2::aes(\n      x = reorder(GC_NAME, !!rlang::sym(plt_var)), \n      y = !!rlang::sym(plt_var),\n      fill = GC_NAME\n    ) +\n    ggplot2::geom_boxplot() +\n    ggplot2::labs(x = \"GC Name\") +\n    vthemes::theme_vmodern() +\n    ggplot2::theme(\n      axis.text.x = ggplot2::element_text(\n        angle = 90, hjust = 1, vjust = 0.5\n      ),\n      legend.position = \"none\"\n    )\n}\nplt &lt;- patchwork::wrap_plots(plt_ls, ncol = 2) +\n  patchwork::plot_layout(axis_titles = \"collect\")\nsubchunkify(\n  plt, fig_height = 30, fig_width = 10, \n  caption = \"'Distribution of abundance values per feature and GC in (mean-imputed) training data.'\"\n)\n\n\n\n\n\n\nDistribution of abundance values per feature and GC in (mean-imputed) training data.\n\n\n\n\n\n\n\n\n\n\n\nGC_NAMEAL_FEC_FECA_FECI_FECO_FECR_FEFE_HK_FEMG_FEMN_FEN_FENA_FENI_FEO_FES_FESI_FETI_FETIII_FEV_FE\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by GC_NAME.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by GC_NAME.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by AL_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by AL_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by C_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by C_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CA_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CA_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CO_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CO_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CR_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CR_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by FE_H.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by FE_H.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by K_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by K_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by MG_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by MG_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by MN_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by MN_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by N_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by N_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by NA_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by NA_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by NI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by NI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by O_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by O_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by S_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by S_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by SI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by SI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by TI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by TI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by TIII_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by TIII_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by V_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by V_FE.\n\n\n\n\n\n\n\n\n\n\nAll GCsBy GCs\n\n\n\nSmallMediumBig\n\n\n\n\n\n\n\nPairwise scatter plot of chemical abundance features in small feature set.\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of chemical abundance features in medium feature set.\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of chemical abundance features in big feature set.\n\n\n\n\n\n\n\n\n\n\nNGC5139NGC0104NGC6656NGC6121NGC6752NGC5272NGC3201NGC2808NGC5904NGC6397NGC6809NGC6218NGC6254NGC0362NGC6273NGC6838NGC0288NGC7078NGC1904NGC6715NGC6205NGC1851NGC6388NGC6341NGC6171NGC7089NGC6544NGC4590NGC5024\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5139 GC (n = 960).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC0104 GC (n = 197).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6656 GC (n = 184).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6121 GC (n = 141).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6752 GC (n = 94).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5272 GC (n = 89).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC3201 GC (n = 88).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC2808 GC (n = 84).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5904 GC (n = 79).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6397 GC (n = 78).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6809 GC (n = 45).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6218 GC (n = 43).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6254 GC (n = 42).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC0362 GC (n = 40).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6273 GC (n = 38).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6838 GC (n = 36).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC0288 GC (n = 33).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC7078 GC (n = 33).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC1904 GC (n = 28).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6715 GC (n = 28).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6205 GC (n = 27).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC1851 GC (n = 24).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6388 GC (n = 24).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6341 GC (n = 22).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6171 GC (n = 20).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC7089 GC (n = 16).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6544 GC (n = 15).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC4590 GC (n = 13).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5024 GC (n = 13).",
    "crumbs": [
      "3 Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "notebooks/02-astro-case-study-eda.html#exploratory-data-analysis",
    "href": "notebooks/02-astro-case-study-eda.html#exploratory-data-analysis",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code\n# get X (using mean-imputed) and metadata for EDA\nmetadata &lt;- metadata$train\nfeature_modes &lt;- list(\"small\" = 7, \"medium\" = 11, \"big\" = 19)\ntrain_data_ls &lt;- purrr::map(\n  feature_modes,\n  ~ get_abundance_data(data_mean_imputed$train, feature_mode = .x)\n)\nfeatures_ls &lt;- purrr::map(train_data_ls, ~ colnames(.x))\nX &lt;- train_data_ls$big\n\n\n\n\n\nOverallBy GC\n\n\n\nShow Code\n# plot overall distribution\nggwrappers::plot_histogram(X) +\n  ggplot2::facet_wrap(~ variable, scales = \"free_x\") +\n  ggplot2::labs(x = \"Data\")\n\n\n\n\nDistribution of abundance values per feature in (mean-imputed) training data.\n\n\n\n\n\nShow Code\n# plot boxplots per GC\nplt_df &lt;- dplyr::bind_cols(\n  X,\n  metadata |&gt; dplyr::select(GC_NAME)\n) |&gt; \n  dplyr::group_by(GC_NAME) |&gt; \n  dplyr::mutate(\n    GC_NAME = sprintf(\"%s (n = %d)\", GC_NAME, dplyr::n())\n  ) |&gt; \n  dplyr::ungroup()\nplt_vars &lt;- sort(colnames(X))\nplt_ls &lt;- list()\nfor (plt_var in plt_vars) {\n  plt_ls[[plt_var]] &lt;- plt_df |&gt; \n    ggplot2::ggplot() +\n    ggplot2::aes(\n      x = reorder(GC_NAME, !!rlang::sym(plt_var)), \n      y = !!rlang::sym(plt_var),\n      fill = GC_NAME\n    ) +\n    ggplot2::geom_boxplot() +\n    ggplot2::labs(x = \"GC Name\") +\n    vthemes::theme_vmodern() +\n    ggplot2::theme(\n      axis.text.x = ggplot2::element_text(\n        angle = 90, hjust = 1, vjust = 0.5\n      ),\n      legend.position = \"none\"\n    )\n}\nplt &lt;- patchwork::wrap_plots(plt_ls, ncol = 2) +\n  patchwork::plot_layout(axis_titles = \"collect\")\nsubchunkify(\n  plt, fig_height = 30, fig_width = 10, \n  caption = \"'Distribution of abundance values per feature and GC in (mean-imputed) training data.'\"\n)\n\n\n\n\n\n\nDistribution of abundance values per feature and GC in (mean-imputed) training data.\n\n\n\n\n\n\n\n\n\n\n\nGC_NAMEAL_FEC_FECA_FECI_FECO_FECR_FEFE_HK_FEMG_FEMN_FEN_FENA_FENI_FEO_FES_FESI_FETI_FETIII_FEV_FE\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by GC_NAME.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by GC_NAME.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by AL_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by AL_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by C_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by C_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CA_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CA_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CO_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CO_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by CR_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by CR_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by FE_H.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by FE_H.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by K_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by K_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by MG_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by MG_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by MN_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by MN_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by N_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by N_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by NA_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by NA_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by NI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by NI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by O_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by O_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by S_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by S_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by SI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by SI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by TI_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by TI_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by TIII_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by TIII_FE.\n\n\n\n\n\n\n\n\n\n\n\nUnjittered scatter plot of the star locations (given by galactic coordinates), colored by V_FE.\n\n\n\n\n\n\n\n\n\nJittered scatter plot of the star locations (given by galactic coordinates), colored by V_FE.\n\n\n\n\n\n\n\n\n\n\nAll GCsBy GCs\n\n\n\nSmallMediumBig\n\n\n\n\n\n\n\nPairwise scatter plot of chemical abundance features in small feature set.\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of chemical abundance features in medium feature set.\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of chemical abundance features in big feature set.\n\n\n\n\n\n\n\n\n\n\nNGC5139NGC0104NGC6656NGC6121NGC6752NGC5272NGC3201NGC2808NGC5904NGC6397NGC6809NGC6218NGC6254NGC0362NGC6273NGC6838NGC0288NGC7078NGC1904NGC6715NGC6205NGC1851NGC6388NGC6341NGC6171NGC7089NGC6544NGC4590NGC5024\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5139 GC (n = 960).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC0104 GC (n = 197).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6656 GC (n = 184).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6121 GC (n = 141).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6752 GC (n = 94).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5272 GC (n = 89).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC3201 GC (n = 88).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC2808 GC (n = 84).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5904 GC (n = 79).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6397 GC (n = 78).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6809 GC (n = 45).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6218 GC (n = 43).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6254 GC (n = 42).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC0362 GC (n = 40).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6273 GC (n = 38).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6838 GC (n = 36).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC0288 GC (n = 33).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC7078 GC (n = 33).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC1904 GC (n = 28).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6715 GC (n = 28).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6205 GC (n = 27).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC1851 GC (n = 24).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6388 GC (n = 24).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6341 GC (n = 22).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6171 GC (n = 20).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC7089 GC (n = 16).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC6544 GC (n = 15).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC4590 GC (n = 13).\n\n\n\n\n\n\n\n\n\n\n\nPairwise scatter plot of all chemical abundance features in NGC5024 GC (n = 13).",
    "crumbs": [
      "3 Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "notebooks/05-astro-case-study-clustering-validation.html",
    "href": "notebooks/05-astro-case-study-clustering-validation.html",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Evaluate Cluster Generalizability using Test Data\n## this code chunk evaluates cluster generalizability using test data\n\n#### choose imputation methods ####\ntest_data_ls &lt;- list(\n  \"Mean-imputed\" = data_mean_imputed$test,\n  \"RF-imputed\" = data_rf_imputed$test\n)\ntrain_data_ls &lt;- list(\n  \"Mean-imputed\" = data_mean_imputed$train,\n  \"RF-imputed\" = data_rf_imputed$train\n)\n\n#### choose number of features ####\nfeature_modes &lt;- list(\n  \"Small\" = 7,\n  \"Medium\" = 11,\n  \"Big\" = 19\n)\n\n#### choose ks ####\nks &lt;- c(2, 8, 9)\n\n#### choose dimension reduction methods ####\n# raw data\nidentity_fun_ls &lt;- list(\"Raw\" = function(x) x)\n\n# pca\npca_fun_ls &lt;- list(\"PCA\" = purrr::partial(fit_pca, ndim = 4))\n\n# tsne\ntsne_perplexities &lt;- c(30, 100)\ntsne_fun_ls &lt;- purrr::map(\n  tsne_perplexities,\n  ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n) |&gt; \n  setNames(sprintf(\"tSNE (perplexity = %d)\", tsne_perplexities))\n\n# putting it together\ndr_fun_ls &lt;- c(\n  identity_fun_ls,\n  pca_fun_ls,\n  tsne_fun_ls\n)\n\n#### choose clustering methods ####\n# kmeans\nkmeans_fun_ls &lt;- list(\"K-means\" = purrr::partial(fit_kmeans, ks = ks))\n\n# spectral clustering\nn_neighbors &lt;- c(60, 100)\nspectral_fun_ls &lt;- purrr::map(\n  n_neighbors,\n  ~ purrr::partial(\n    fit_spectral_clustering, \n    ks = ks,\n    affinity = \"nearest_neighbors\",\n    n_neighbors = .x\n  )\n) |&gt; \n  setNames(sprintf(\"Spectral (n_neighbors = %s)\", n_neighbors))\n\n# putting it together\nclust_fun_ls &lt;- c(\n  kmeans_fun_ls,\n  spectral_fun_ls\n)\n\n#### Fit Clustering Pipelines ####\npipe_tib &lt;- tidyr::expand_grid(\n  train_data = train_data_ls,\n  test_data = test_data_ls,\n  feature_mode = feature_modes,\n  dr_method = dr_fun_ls,\n  clust_method = clust_fun_ls\n) |&gt; \n  dplyr::mutate(\n    impute_mode_name = names(train_data),\n    feature_mode_name = names(feature_mode),\n    dr_method_name = names(dr_method),\n    clust_method_name = names(clust_method),\n    name = stringr::str_glue(\n      \"{clust_method_name} [{impute_mode_name} + {feature_mode_name} + {dr_method_name}]\"\n    )\n  ) |&gt; \n  # remove some clustering pipelines to reduce computation burden\n  dplyr::filter(\n    # remove all big feature set + dimension-reduction runs\n    !((dr_method_name != \"Raw\") & (feature_mode_name == \"Big\"))\n  )\npipe_ls &lt;- split(pipe_tib, seq_len(nrow(pipe_tib))) |&gt; \n  setNames(pipe_tib$name)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"clustering_fits_test.rds\")\nif (!file.exists(fit_results_fname)) {\n  library(future)\n  plan(multisession, workers = NCORES)\n  \n  # fit clustering pipelines (if not already cached)\n  clust_fit_ls &lt;- furrr::future_map(\n    pipe_ls,\n    function(pipe_df) {\n      g &lt;- create_preprocessing_pipeline(\n        feature_mode = pipe_df$feature_mode[[1]],\n        preprocess_fun = pipe_df$dr_method[[1]]\n      )\n      clust_out &lt;- pipe_df$clust_method[[1]](\n        data = pipe_df$test_data[[1]], preprocess_fun = g\n      )\n      return(clust_out)\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        ks = ks,\n        create_preprocessing_pipeline = create_preprocessing_pipeline,\n        get_abundance_data = get_abundance_data,\n        tsne_perplexities = tsne_perplexities,\n        n_neighbors = n_neighbors,\n        fit_kmeans = fit_kmeans,\n        fit_spectral_clustering = fit_spectral_clustering\n      )\n    )\n  )\n  # save fitted clustering pipelines\n  saveRDS(clust_fit_ls, file = fit_results_fname)\n  \n  # evaluate generalizability of test clusters\n  clust_fit_ls &lt;- clust_fit_ls |&gt; \n    purrr::map(~ .x$cluster_ids) |&gt; \n    purrr::list_flatten(name_spec = \"{inner}: {outer}\")\n  clust_fit_df &lt;- tibble::tibble(\n    name = names(clust_fit_ls),\n    cluster_ids = clust_fit_ls\n  ) |&gt; \n    annotate_clustering_results() |&gt; \n    dplyr::group_by(k, clust_method_name) |&gt; \n    dplyr::summarise(\n      cluster_list = list(cluster_ids),\n      .groups = \"drop\"\n    )\n  \n  train_consensus_out_ls &lt;- readRDS(\n    file.path(RESULTS_PATH, \"consensus_clusters_train.rds\")\n  )\n  \n  test_nbhd_mat_ls &lt;- list()\n  test_consensus_out_ls &lt;- list()\n  gen_errs &lt;- list()\n  for (i in 1:length(best_clust_methods)) {\n    clust_method_name &lt;- best_clust_methods[[i]]$clust_method_name\n    k &lt;- best_clust_methods[[i]]$k\n    key &lt;- names(best_clust_methods)[i]\n    \n    keep_clust_ls &lt;- clust_fit_df |&gt; \n      dplyr::filter(\n        k == !!k,\n        clust_method_name == !!clust_method_name\n      ) |&gt; \n      dplyr::pull(cluster_list)\n    keep_clust_ls &lt;- keep_clust_ls[[1]]\n    \n    # compute neighborhood matrix\n    test_nbhd_mat_ls[[key]] &lt;- get_consensus_neighborhood_matrix(keep_clust_ls)\n    # aggregate stable clusters using consensus clustering\n    test_nbhd_mat &lt;- test_nbhd_mat_ls[[key]]\n    test_consensus_out_ls[[key]] &lt;- fit_consensus_clusters(test_nbhd_mat, k = k)\n    \n    # assess generalizability\n    data_pipe_tib &lt;- pipe_tib |&gt; \n      dplyr::distinct(impute_mode_name, feature_mode_name, .keep_all = TRUE) |&gt; \n      dplyr::mutate(\n        name = sprintf(\"%s + %s\", impute_mode_name, feature_mode_name)\n      )\n    data_pipe_ls &lt;- split(data_pipe_tib, seq_len(nrow(data_pipe_tib))) |&gt; \n      setNames(data_pipe_tib$name)\n    gen_errs[[key]] &lt;- purrr::map(\n      data_pipe_ls,\n      function(data_pipe_df) {\n        g &lt;- create_preprocessing_pipeline(\n          feature_mode = data_pipe_df$feature_mode[[1]],\n          preprocess_fun = identity_fun_ls[[1]]\n        )\n        X_train &lt;- g(data_pipe_df$train_data[[1]])\n        X_test &lt;- g(data_pipe_df$test_data[[1]])\n        cluster_generalizability(\n          X_train = X_train,\n          cluster_train = train_consensus_out_ls[[key]]$cluster_ids,\n          X_test = X_test,\n          cluster_test = test_consensus_out_ls[[key]]$cluster_ids\n        )\n      }\n    )\n  }\n  saveRDS(\n    test_consensus_out_ls,\n    file.path(RESULTS_PATH, \"consensus_clusters_test.rds\")\n  )\n  saveRDS(\n    test_nbhd_mat_ls,\n    file.path(RESULTS_PATH, \"consensus_neighborhood_matrices_test.rds\")\n  )\n  saveRDS(\n    gen_errs,\n    file.path(RESULTS_PATH, \"generalizability_errors_test.rds\")\n  )\n} else {\n  # read in results (if already cached)\n  gen_errs &lt;- readRDS(\n    file.path(RESULTS_PATH, \"generalizability_errors_test.rds\")\n  )\n}\n\n\n\nCluster GeneralizabilityConfusion Tables\n\n\n\n\n\n\n\n\nOverall generalizability, computed as the ARI between the estimated test cluster labels and the predicted cluster labels.\n\n\n\n\n\n\n\n\n\nLocal generalizability, computed as the largest overlap between the predicted clusters and the estimated test clusters divided by the number of samples in the predicted cluster.\n\n\n\n\n\n\nMean-imputed + SmallMean-imputed + MediumMean-imputed + BigRF-imputed + SmallRF-imputed + MediumRF-imputed + Big",
    "crumbs": [
      "6 Clustering (validation)"
    ]
  },
  {
    "objectID": "notebooks/05-astro-case-study-clustering-validation.html#validation-cluster-generalizability",
    "href": "notebooks/05-astro-case-study-clustering-validation.html#validation-cluster-generalizability",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Evaluate Cluster Generalizability using Test Data\n## this code chunk evaluates cluster generalizability using test data\n\n#### choose imputation methods ####\ntest_data_ls &lt;- list(\n  \"Mean-imputed\" = data_mean_imputed$test,\n  \"RF-imputed\" = data_rf_imputed$test\n)\ntrain_data_ls &lt;- list(\n  \"Mean-imputed\" = data_mean_imputed$train,\n  \"RF-imputed\" = data_rf_imputed$train\n)\n\n#### choose number of features ####\nfeature_modes &lt;- list(\n  \"Small\" = 7,\n  \"Medium\" = 11,\n  \"Big\" = 19\n)\n\n#### choose ks ####\nks &lt;- c(2, 8, 9)\n\n#### choose dimension reduction methods ####\n# raw data\nidentity_fun_ls &lt;- list(\"Raw\" = function(x) x)\n\n# pca\npca_fun_ls &lt;- list(\"PCA\" = purrr::partial(fit_pca, ndim = 4))\n\n# tsne\ntsne_perplexities &lt;- c(30, 100)\ntsne_fun_ls &lt;- purrr::map(\n  tsne_perplexities,\n  ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n) |&gt; \n  setNames(sprintf(\"tSNE (perplexity = %d)\", tsne_perplexities))\n\n# putting it together\ndr_fun_ls &lt;- c(\n  identity_fun_ls,\n  pca_fun_ls,\n  tsne_fun_ls\n)\n\n#### choose clustering methods ####\n# kmeans\nkmeans_fun_ls &lt;- list(\"K-means\" = purrr::partial(fit_kmeans, ks = ks))\n\n# spectral clustering\nn_neighbors &lt;- c(60, 100)\nspectral_fun_ls &lt;- purrr::map(\n  n_neighbors,\n  ~ purrr::partial(\n    fit_spectral_clustering, \n    ks = ks,\n    affinity = \"nearest_neighbors\",\n    n_neighbors = .x\n  )\n) |&gt; \n  setNames(sprintf(\"Spectral (n_neighbors = %s)\", n_neighbors))\n\n# putting it together\nclust_fun_ls &lt;- c(\n  kmeans_fun_ls,\n  spectral_fun_ls\n)\n\n#### Fit Clustering Pipelines ####\npipe_tib &lt;- tidyr::expand_grid(\n  train_data = train_data_ls,\n  test_data = test_data_ls,\n  feature_mode = feature_modes,\n  dr_method = dr_fun_ls,\n  clust_method = clust_fun_ls\n) |&gt; \n  dplyr::mutate(\n    impute_mode_name = names(train_data),\n    feature_mode_name = names(feature_mode),\n    dr_method_name = names(dr_method),\n    clust_method_name = names(clust_method),\n    name = stringr::str_glue(\n      \"{clust_method_name} [{impute_mode_name} + {feature_mode_name} + {dr_method_name}]\"\n    )\n  ) |&gt; \n  # remove some clustering pipelines to reduce computation burden\n  dplyr::filter(\n    # remove all big feature set + dimension-reduction runs\n    !((dr_method_name != \"Raw\") & (feature_mode_name == \"Big\"))\n  )\npipe_ls &lt;- split(pipe_tib, seq_len(nrow(pipe_tib))) |&gt; \n  setNames(pipe_tib$name)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"clustering_fits_test.rds\")\nif (!file.exists(fit_results_fname)) {\n  library(future)\n  plan(multisession, workers = NCORES)\n  \n  # fit clustering pipelines (if not already cached)\n  clust_fit_ls &lt;- furrr::future_map(\n    pipe_ls,\n    function(pipe_df) {\n      g &lt;- create_preprocessing_pipeline(\n        feature_mode = pipe_df$feature_mode[[1]],\n        preprocess_fun = pipe_df$dr_method[[1]]\n      )\n      clust_out &lt;- pipe_df$clust_method[[1]](\n        data = pipe_df$test_data[[1]], preprocess_fun = g\n      )\n      return(clust_out)\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        ks = ks,\n        create_preprocessing_pipeline = create_preprocessing_pipeline,\n        get_abundance_data = get_abundance_data,\n        tsne_perplexities = tsne_perplexities,\n        n_neighbors = n_neighbors,\n        fit_kmeans = fit_kmeans,\n        fit_spectral_clustering = fit_spectral_clustering\n      )\n    )\n  )\n  # save fitted clustering pipelines\n  saveRDS(clust_fit_ls, file = fit_results_fname)\n  \n  # evaluate generalizability of test clusters\n  clust_fit_ls &lt;- clust_fit_ls |&gt; \n    purrr::map(~ .x$cluster_ids) |&gt; \n    purrr::list_flatten(name_spec = \"{inner}: {outer}\")\n  clust_fit_df &lt;- tibble::tibble(\n    name = names(clust_fit_ls),\n    cluster_ids = clust_fit_ls\n  ) |&gt; \n    annotate_clustering_results() |&gt; \n    dplyr::group_by(k, clust_method_name) |&gt; \n    dplyr::summarise(\n      cluster_list = list(cluster_ids),\n      .groups = \"drop\"\n    )\n  \n  train_consensus_out_ls &lt;- readRDS(\n    file.path(RESULTS_PATH, \"consensus_clusters_train.rds\")\n  )\n  \n  test_nbhd_mat_ls &lt;- list()\n  test_consensus_out_ls &lt;- list()\n  gen_errs &lt;- list()\n  for (i in 1:length(best_clust_methods)) {\n    clust_method_name &lt;- best_clust_methods[[i]]$clust_method_name\n    k &lt;- best_clust_methods[[i]]$k\n    key &lt;- names(best_clust_methods)[i]\n    \n    keep_clust_ls &lt;- clust_fit_df |&gt; \n      dplyr::filter(\n        k == !!k,\n        clust_method_name == !!clust_method_name\n      ) |&gt; \n      dplyr::pull(cluster_list)\n    keep_clust_ls &lt;- keep_clust_ls[[1]]\n    \n    # compute neighborhood matrix\n    test_nbhd_mat_ls[[key]] &lt;- get_consensus_neighborhood_matrix(keep_clust_ls)\n    # aggregate stable clusters using consensus clustering\n    test_nbhd_mat &lt;- test_nbhd_mat_ls[[key]]\n    test_consensus_out_ls[[key]] &lt;- fit_consensus_clusters(test_nbhd_mat, k = k)\n    \n    # assess generalizability\n    data_pipe_tib &lt;- pipe_tib |&gt; \n      dplyr::distinct(impute_mode_name, feature_mode_name, .keep_all = TRUE) |&gt; \n      dplyr::mutate(\n        name = sprintf(\"%s + %s\", impute_mode_name, feature_mode_name)\n      )\n    data_pipe_ls &lt;- split(data_pipe_tib, seq_len(nrow(data_pipe_tib))) |&gt; \n      setNames(data_pipe_tib$name)\n    gen_errs[[key]] &lt;- purrr::map(\n      data_pipe_ls,\n      function(data_pipe_df) {\n        g &lt;- create_preprocessing_pipeline(\n          feature_mode = data_pipe_df$feature_mode[[1]],\n          preprocess_fun = identity_fun_ls[[1]]\n        )\n        X_train &lt;- g(data_pipe_df$train_data[[1]])\n        X_test &lt;- g(data_pipe_df$test_data[[1]])\n        cluster_generalizability(\n          X_train = X_train,\n          cluster_train = train_consensus_out_ls[[key]]$cluster_ids,\n          X_test = X_test,\n          cluster_test = test_consensus_out_ls[[key]]$cluster_ids\n        )\n      }\n    )\n  }\n  saveRDS(\n    test_consensus_out_ls,\n    file.path(RESULTS_PATH, \"consensus_clusters_test.rds\")\n  )\n  saveRDS(\n    test_nbhd_mat_ls,\n    file.path(RESULTS_PATH, \"consensus_neighborhood_matrices_test.rds\")\n  )\n  saveRDS(\n    gen_errs,\n    file.path(RESULTS_PATH, \"generalizability_errors_test.rds\")\n  )\n} else {\n  # read in results (if already cached)\n  gen_errs &lt;- readRDS(\n    file.path(RESULTS_PATH, \"generalizability_errors_test.rds\")\n  )\n}\n\n\n\nCluster GeneralizabilityConfusion Tables\n\n\n\n\n\n\n\n\nOverall generalizability, computed as the ARI between the estimated test cluster labels and the predicted cluster labels.\n\n\n\n\n\n\n\n\n\nLocal generalizability, computed as the largest overlap between the predicted clusters and the estimated test clusters divided by the number of samples in the predicted cluster.\n\n\n\n\n\n\nMean-imputed + SmallMean-imputed + MediumMean-imputed + BigRF-imputed + SmallRF-imputed + MediumRF-imputed + Big",
    "crumbs": [
      "6 Clustering (validation)"
    ]
  },
  {
    "objectID": "notebooks/05-astro-case-study-clustering-validation.html#validation-stability-across-alternative-data-preprocessing",
    "href": "notebooks/05-astro-case-study-clustering-validation.html#validation-stability-across-alternative-data-preprocessing",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "Validation: Stability across Alternative Data Preprocessing",
    "text": "Validation: Stability across Alternative Data Preprocessing\n\nShow Code to Evaluate Cluster Stability Across Alternative Data Preprocessing Pipelines\n# Default QC thresholds\ndefault_grid &lt;- list(\n  snr = 70,\n  vb = 0.9,\n  logg = 3.6,\n  teff_width = 1000  # yields [3500, 5500]\n)\n\n# Parameter grids to sweep\nvary_param_grid &lt;- list(\n  snr = c(30, 50, 70, 90, 110, 130, 150),\n  vb = c(0.5, 0.7, 0.9, 0.99),\n  logg = c(3.0, 3.3, 3.6, 3.9, 4.2),\n  teff_width = c(500, 1000, 1500, 2000)\n)\n\n# Load data\ndata_mean_imputed_train &lt;- dplyr::bind_cols(\n  metadata$train, data_mean_imputed$train\n)\ndata_rf_imputed_train &lt;- dplyr::bind_cols(\n  metadata$train, data_rf_imputed$train\n)\n\n# Get original cluster fit\norig_clusters &lt;- readRDS(\n  file.path(RESULTS_PATH, \"consensus_clusters_train.rds\")\n)[[best_clust_method_name]]$cluster_ids\n\nstability_results_fname &lt;- file.path(\n  RESULTS_PATH, \"clustering_fits_stability.rds\"\n)\n\nif (!file.exists(stability_results_fname)) {\n  stability_fits_ls &lt;- list()\n  for (param_name in names(vary_param_grid)) {\n    param_grid_ls &lt;- default_grid\n    param_grid_ls[[param_name]] &lt;- vary_param_grid[[param_name]]\n    param_grid &lt;- expand.grid(param_grid_ls)\n    stability_fits_ls[[param_name]] &lt;- purrr::map(\n      1:nrow(param_grid),\n      function(i) {\n        snr &lt;- param_grid$snr[[i]]\n        vb &lt;- param_grid$vb[[i]]\n        logg &lt;- param_grid$logg[[i]]\n        teff_width &lt;- param_grid$teff_width[[i]]\n        \n        # get new QC-filtered data\n        data_mean_imputed_qc &lt;- apply_qc_filtering(\n          data_mean_imputed_train, \n          snr_threshold = snr,\n          teff_thresholds = c(4500 - teff_width, 4500 + teff_width),\n          logg_threshold = logg,\n          starflag = 0,\n          vb_threshold = vb\n        )\n        data_rf_imputed_qc &lt;- apply_qc_filtering(\n          data_rf_imputed_train, \n          snr_threshold = snr,\n          teff_thresholds = c(4500 - teff_width, 4500 + teff_width),\n          logg_threshold = logg,\n          starflag = 0,\n          vb_threshold = vb\n        )\n        \n        # get sample ids for new QC-filtered data\n        keep_sample_idxs &lt;- tibble::tibble(\n          APOGEE_ID = data_mean_imputed_qc$meta$APOGEE_ID\n        ) |&gt; \n          dplyr::left_join(\n            dplyr::bind_cols(.id = 1:nrow(metadata$train), metadata$train),\n            by = \"APOGEE_ID\"\n          ) |&gt; \n          dplyr::pull(.id)\n        \n        # fit clustering pipeline on new QC-filtered data\n        cluster_ids &lt;- fit_best_pipeline(\n          data_ls = list(\n            \"Mean-imputed\" = data_mean_imputed_qc$abundance,\n            \"RF-imputed\" = data_rf_imputed_qc$abundance\n          ),\n          k = best_k\n        )\n        \n        # evaluate stability between original clusters and new clusters\n        stability &lt;- mclust::adjustedRandIndex(\n          cluster_ids, orig_clusters[keep_sample_idxs]\n        )\n        \n        return(list(\n          sample_ids = keep_sample_idxs,\n          cluster_ids = cluster_ids,\n          stability = stability\n        ))\n      }\n    ) |&gt; \n      setNames(vary_param_grid[[param_name]])\n  }\n  saveRDS(stability_fits_ls, stability_results_fname)\n} else {\n  stability_fits_ls &lt;- readRDS(stability_results_fname)\n}\n\nstability_results_ls &lt;- purrr::imap(\n  stability_fits_ls,\n  function(x, key) {\n    purrr::map(x, ~ data.frame(stability = .x$stability)) |&gt; \n      dplyr::bind_rows(.id = key) |&gt; \n      dplyr::mutate(\n        {{key}} := as.numeric(.data[[key]])\n      )\n  }\n)\n\nplt_ls &lt;- purrr::imap(\n  stability_results_ls,\n  function(stability_df, key) {\n    param_title &lt;- dplyr::case_when(\n      key == \"snr\" ~ \"SNR Threshold\",\n      key == \"vb\" ~ \"VB Threshold\",\n      key == \"logg\" ~ \"Log(g) Threshold\",\n      key == \"teff_width\" ~ \"Teff Width\"\n    )\n    stability_df |&gt; \n      ggplot2::ggplot() +\n      ggplot2::aes(x = !!rlang::sym(key), y = stability) +\n      ggplot2::geom_line(width = 1.2, color = \"steelblue\") +\n      ggplot2::geom_point(width = 2.5, color = \"steelblue\") +\n      ggplot2::ylim(c(0, 1)) +\n      ggplot2::labs(\n        x = param_title,\n        y = \"ARI Stability\",\n      ) +\n      ggplot2::theme_minimal(base_size = 14) +\n      ggplot2::theme(\n        panel.grid.major = ggplot2::element_line(color = \"grey80\"),\n        panel.grid.minor = ggplot2::element_blank(),\n        axis.title.y.left = ggplot2::element_text(color = \"steelblue\")\n      )\n  }\n)\n\nplt &lt;- patchwork::wrap_plots(plt_ls, nrow = 1) +\n  patchwork::plot_layout(axes = \"collect\")\nsubchunkify(\n  plt, fig_height = 4, fig_width = 10,\n  caption = \"'Stability of cluster labels across different choices of data preprocessing pipelines.'\"\n)\n\n\n\n\n\n\nStability of cluster labels across different choices of data preprocessing pipelines.",
    "crumbs": [
      "6 Clustering (validation)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "The emergence of large spectroscopic surveys of the Milky Way has led to significant interest in studying the chemical origins of the Galaxy’s formation. In particular, researchers are often interested in understanding how stars were formed or evolved chemodynamically over large periods of time. To this end, previous work has shown that in the process of forming a stellar body, parent molecular clouds can produce hundreds of stars in a single burst (Krumholz et al. 2014), but due to astronomical dynamics, these shared origins are challenging to find.\nIn this case study, we will leverage the Apache Point Observatory Galactic Evolution Experiment (APOGEE) DR17 (Prieto et al. 2008), a large high-resolution spectroscopic survey of stars compring the disk (i.e., the primary area of the Milky Way’s stellar mass), in order to identify groups of stars in the Milky Way with similar chemical properties and gain insights into the shared origins of stars that were formed together.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "The emergence of large spectroscopic surveys of the Milky Way has led to significant interest in studying the chemical origins of the Galaxy’s formation. In particular, researchers are often interested in understanding how stars were formed or evolved chemodynamically over large periods of time. To this end, previous work has shown that in the process of forming a stellar body, parent molecular clouds can produce hundreds of stars in a single burst (Krumholz et al. 2014), but due to astronomical dynamics, these shared origins are challenging to find.\nIn this case study, we will leverage the Apache Point Observatory Galactic Evolution Experiment (APOGEE) DR17 (Prieto et al. 2008), a large high-resolution spectroscopic survey of stars compring the disk (i.e., the primary area of the Milky Way’s stellar mass), in order to identify groups of stars in the Milky Way with similar chemical properties and gain insights into the shared origins of stars that were formed together.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "Outline",
    "text": "Outline\nIn what follows, we will walk through a typical unsupervised machine learning workflow for scientific discovery, namely, to identify common origins of stars in the Milky Way. To this end, we will proceed through the following steps:\n\nData Preparation and Cleaning: We begin by loading in the data, performing some basic quality control filtering and cleaning, and splitting the data into a training and test set.\nExploratory Data Analysis: We then conduct a brief exploratory data analysis to better understand various characteristics of the data.\nDimension Reduction: We further implement various dimension reduction techniques (and tune their hyperparameters) to both visualize the data in a lower-dimensional space and to prepare for clustering.\nClustering (training): Next, we fit various clustering techniques on multiple versions of the training data (e.g., using different ways of preparing the data) and perform model selection and hyperparameter tuning based upon the stability of the resulting clusters.\nClustering (validation): We then validate the clustering results on the test set (e.g., via generalizability metrics and its stability across alternative data preprocessing pipelines).\nInterpretation of Clustering Results: Finally, we re-fit the best clustering pipeline on the full data and interpret the final clusters in the context of the scientific question at hand.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "notebooks/06-astro-case-study-clustering-interpretation.html",
    "href": "notebooks/06-astro-case-study-clustering-interpretation.html",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Fit Final (tuned) Clustering Pipeline on Full Data\n## this code chunk fits the final (tuned) clustering pipeline on the full data\n\n#### choose imputation methods ####\ndata_ls &lt;- list(\n  \"Mean-imputed\" = rbind(data_mean_imputed$train, data_mean_imputed$test),\n  \"RF-imputed\" = rbind(data_rf_imputed$train, data_rf_imputed$test)\n)\n\n#### choose number of features ####\nfeature_modes &lt;- list(\n  \"Small\" = 7,\n  \"Medium\" = 11,\n  \"Big\" = 19\n)\n\n#### choose dimension reduction methods ####\n# raw data\nidentity_fun_ls &lt;- list(\"Raw\" = function(x) x)\n\n# pca\npca_fun_ls &lt;- list(\"PCA\" = purrr::partial(fit_pca, ndim = 4))\n\n# tsne\ntsne_perplexities &lt;- c(30, 100)\ntsne_fun_ls &lt;- purrr::map(\n  tsne_perplexities,\n  ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n) |&gt; \n  setNames(sprintf(\"tSNE (perplexity = %d)\", tsne_perplexities))\n\n# putting it together\ndr_fun_ls &lt;- c(\n  identity_fun_ls,\n  pca_fun_ls,\n  tsne_fun_ls\n)\n\n#### choose clustering methods ####\n# kmeans\nkmeans_fun_ls &lt;- list(\"K-means\" = purrr::partial(fit_kmeans, ks = ks))\n\n# spectral clustering\nn_neighbors &lt;- c(60, 100)\nspectral_fun_ls &lt;- purrr::map(\n  n_neighbors,\n  ~ purrr::partial(\n    fit_spectral_clustering, \n    ks = ks,\n    affinity = \"nearest_neighbors\",\n    n_neighbors = .x\n  )\n) |&gt; \n  setNames(sprintf(\"Spectral (n_neighbors = %s)\", n_neighbors))\n\n# putting it together\nclust_fun_ls &lt;- c(\n  kmeans_fun_ls,\n  spectral_fun_ls\n)\n\n#### Fit Clustering Pipelines ####\npipe_tib &lt;- tidyr::expand_grid(\n  data = data_ls,\n  feature_mode = feature_modes,\n  dr_method = dr_fun_ls,\n  clust_method = clust_fun_ls\n) |&gt; \n  dplyr::mutate(\n    impute_mode_name = names(data),\n    feature_mode_name = names(feature_mode),\n    dr_method_name = names(dr_method),\n    clust_method_name = names(clust_method),\n    name = stringr::str_glue(\n      \"{clust_method_name} [{impute_mode_name} + {feature_mode_name} + {dr_method_name}]\"\n    )\n  ) |&gt; \n  # remove some clustering pipelines to reduce computation burden\n  dplyr::filter(\n    # remove all big feature set + dimension-reduction runs\n    !((dr_method_name != \"Raw\") & (feature_mode_name == \"Big\")),\n    # restrict to tuned models\n    clust_method_name == !!best_clust_method_name\n  )\npipe_ls &lt;- split(pipe_tib, seq_len(nrow(pipe_tib))) |&gt; \n  setNames(pipe_tib$name)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"clustering_fits_final.rds\")\nconsensus_clusters_results_path &lt;- file.path(\n  RESULTS_PATH, \"consensus_clusters_final.rds\"\n)\nconsensus_nbhd_results_path &lt;- file.path(\n  RESULTS_PATH, \"consensus_neighborhood_matrices_final.rds\"\n)\nif (!file.exists(fit_results_fname) ||\n    !file.exists(consensus_clusters_results_path) ||\n    !file.exists(consensus_nbhd_results_path)) {\n  library(future)\n  plan(multisession, workers = NCORES)\n  \n  # fit clustering pipelines (if not already cached)\n  clust_fit_ls &lt;- furrr::future_map(\n    pipe_ls,\n    function(pipe_df) {\n      g &lt;- create_preprocessing_pipeline(\n        feature_mode = pipe_df$feature_mode[[1]],\n        preprocess_fun = pipe_df$dr_method[[1]]\n      )\n      clust_out &lt;- pipe_df$clust_method[[1]](\n        data = pipe_df$data[[1]], preprocess_fun = g\n      )\n      return(clust_out)\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        ks = best_k,\n        create_preprocessing_pipeline = create_preprocessing_pipeline,\n        get_abundance_data = get_abundance_data,\n        tsne_perplexities = tsne_perplexities,\n        n_neighbors = n_neighbors,\n        fit_kmeans = fit_kmeans,\n        fit_spectral_clustering = fit_spectral_clustering\n      )\n    )\n  )\n  # save fitted clustering pipelines\n  saveRDS(clust_fit_ls, file = fit_results_fname)\n  \n  # estimate consensus clusters\n  clust_fit_ls &lt;- purrr::map(clust_fit_ls, ~ .x$cluster_ids) |&gt; \n    purrr::list_flatten(name_spec = \"{inner}: {outer}\")\n  nbhd_mat &lt;- get_consensus_neighborhood_matrix(clust_fit_ls)\n  consensus_out &lt;- fit_consensus_clusters(nbhd_mat, k = best_k)\n  saveRDS(consensus_out, file = consensus_clusters_results_path)\n  saveRDS(nbhd_mat, file = consensus_nbhd_results_path)\n} else {\n  # read in results (if already cached)\n  clust_fit_ls &lt;- readRDS(fit_results_fname)\n  consensus_out &lt;- readRDS(consensus_clusters_results_path)\n  nbhd_mat &lt;- readRDS(consensus_nbhd_results_path)\n}",
    "crumbs": [
      "7 Intepreting Final Clusters"
    ]
  },
  {
    "objectID": "notebooks/06-astro-case-study-clustering-interpretation.html#fit-tuned-clustering-pipelines-on-full-data",
    "href": "notebooks/06-astro-case-study-clustering-interpretation.html#fit-tuned-clustering-pipelines-on-full-data",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Fit Final (tuned) Clustering Pipeline on Full Data\n## this code chunk fits the final (tuned) clustering pipeline on the full data\n\n#### choose imputation methods ####\ndata_ls &lt;- list(\n  \"Mean-imputed\" = rbind(data_mean_imputed$train, data_mean_imputed$test),\n  \"RF-imputed\" = rbind(data_rf_imputed$train, data_rf_imputed$test)\n)\n\n#### choose number of features ####\nfeature_modes &lt;- list(\n  \"Small\" = 7,\n  \"Medium\" = 11,\n  \"Big\" = 19\n)\n\n#### choose dimension reduction methods ####\n# raw data\nidentity_fun_ls &lt;- list(\"Raw\" = function(x) x)\n\n# pca\npca_fun_ls &lt;- list(\"PCA\" = purrr::partial(fit_pca, ndim = 4))\n\n# tsne\ntsne_perplexities &lt;- c(30, 100)\ntsne_fun_ls &lt;- purrr::map(\n  tsne_perplexities,\n  ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n) |&gt; \n  setNames(sprintf(\"tSNE (perplexity = %d)\", tsne_perplexities))\n\n# putting it together\ndr_fun_ls &lt;- c(\n  identity_fun_ls,\n  pca_fun_ls,\n  tsne_fun_ls\n)\n\n#### choose clustering methods ####\n# kmeans\nkmeans_fun_ls &lt;- list(\"K-means\" = purrr::partial(fit_kmeans, ks = ks))\n\n# spectral clustering\nn_neighbors &lt;- c(60, 100)\nspectral_fun_ls &lt;- purrr::map(\n  n_neighbors,\n  ~ purrr::partial(\n    fit_spectral_clustering, \n    ks = ks,\n    affinity = \"nearest_neighbors\",\n    n_neighbors = .x\n  )\n) |&gt; \n  setNames(sprintf(\"Spectral (n_neighbors = %s)\", n_neighbors))\n\n# putting it together\nclust_fun_ls &lt;- c(\n  kmeans_fun_ls,\n  spectral_fun_ls\n)\n\n#### Fit Clustering Pipelines ####\npipe_tib &lt;- tidyr::expand_grid(\n  data = data_ls,\n  feature_mode = feature_modes,\n  dr_method = dr_fun_ls,\n  clust_method = clust_fun_ls\n) |&gt; \n  dplyr::mutate(\n    impute_mode_name = names(data),\n    feature_mode_name = names(feature_mode),\n    dr_method_name = names(dr_method),\n    clust_method_name = names(clust_method),\n    name = stringr::str_glue(\n      \"{clust_method_name} [{impute_mode_name} + {feature_mode_name} + {dr_method_name}]\"\n    )\n  ) |&gt; \n  # remove some clustering pipelines to reduce computation burden\n  dplyr::filter(\n    # remove all big feature set + dimension-reduction runs\n    !((dr_method_name != \"Raw\") & (feature_mode_name == \"Big\")),\n    # restrict to tuned models\n    clust_method_name == !!best_clust_method_name\n  )\npipe_ls &lt;- split(pipe_tib, seq_len(nrow(pipe_tib))) |&gt; \n  setNames(pipe_tib$name)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"clustering_fits_final.rds\")\nconsensus_clusters_results_path &lt;- file.path(\n  RESULTS_PATH, \"consensus_clusters_final.rds\"\n)\nconsensus_nbhd_results_path &lt;- file.path(\n  RESULTS_PATH, \"consensus_neighborhood_matrices_final.rds\"\n)\nif (!file.exists(fit_results_fname) ||\n    !file.exists(consensus_clusters_results_path) ||\n    !file.exists(consensus_nbhd_results_path)) {\n  library(future)\n  plan(multisession, workers = NCORES)\n  \n  # fit clustering pipelines (if not already cached)\n  clust_fit_ls &lt;- furrr::future_map(\n    pipe_ls,\n    function(pipe_df) {\n      g &lt;- create_preprocessing_pipeline(\n        feature_mode = pipe_df$feature_mode[[1]],\n        preprocess_fun = pipe_df$dr_method[[1]]\n      )\n      clust_out &lt;- pipe_df$clust_method[[1]](\n        data = pipe_df$data[[1]], preprocess_fun = g\n      )\n      return(clust_out)\n    },\n    .options = furrr::furrr_options(\n      seed = TRUE, \n      globals = list(\n        ks = best_k,\n        create_preprocessing_pipeline = create_preprocessing_pipeline,\n        get_abundance_data = get_abundance_data,\n        tsne_perplexities = tsne_perplexities,\n        n_neighbors = n_neighbors,\n        fit_kmeans = fit_kmeans,\n        fit_spectral_clustering = fit_spectral_clustering\n      )\n    )\n  )\n  # save fitted clustering pipelines\n  saveRDS(clust_fit_ls, file = fit_results_fname)\n  \n  # estimate consensus clusters\n  clust_fit_ls &lt;- purrr::map(clust_fit_ls, ~ .x$cluster_ids) |&gt; \n    purrr::list_flatten(name_spec = \"{inner}: {outer}\")\n  nbhd_mat &lt;- get_consensus_neighborhood_matrix(clust_fit_ls)\n  consensus_out &lt;- fit_consensus_clusters(nbhd_mat, k = best_k)\n  saveRDS(consensus_out, file = consensus_clusters_results_path)\n  saveRDS(nbhd_mat, file = consensus_nbhd_results_path)\n} else {\n  # read in results (if already cached)\n  clust_fit_ls &lt;- readRDS(fit_results_fname)\n  consensus_out &lt;- readRDS(consensus_clusters_results_path)\n  nbhd_mat &lt;- readRDS(consensus_nbhd_results_path)\n}",
    "crumbs": [
      "7 Intepreting Final Clusters"
    ]
  },
  {
    "objectID": "notebooks/06-astro-case-study-clustering-interpretation.html#interpreting-the-final-clusters",
    "href": "notebooks/06-astro-case-study-clustering-interpretation.html#interpreting-the-final-clusters",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "Interpreting the Final Clusters",
    "text": "Interpreting the Final Clusters\n\nk = 8: K-means\n\n\n\nOverviewLocal StabilityAbundance per Cluster\n\n\n\nConsensus Neighborhood Heatmap\n\n\n\n\n\nConsensus neighborhood heatmap of the final clustering pipeline.\n\n\n\n\n\n\nGCs per Cluster\n\n\n\n\nGC composition for each estimated consensus cluster.\n\n\n\n\n\n\nOn Galactic Coordinates\n\n\n\n\n\nGalactic coordinates of stars, colored by its local cluster stability.\n\n\n\n\n\n\nOn Dimension Reduction\n\n\n\n\n\nDimension reduction visualizations of stars, colored by its local cluster stability.\n\n\n\n\n\n\n\n\n\n\n\nAbundance of features per estimated consensus cluster.",
    "crumbs": [
      "7 Intepreting Final Clusters"
    ]
  },
  {
    "objectID": "notebooks/01-astro-case-study-data.html",
    "href": "notebooks/01-astro-case-study-data.html",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Let us begin by loading in the APOGEE DR17 data, which was originally downloaded from SDSS (or direct download link here).\n\n\nShow Code\n# Load data\ndata_orig &lt;- load_astro_data(data_dir = DATA_PATH)\n\n\nThis data contains both the chemical abundance data and metadata about the quality control and star properties.\nA detailed data dictionary can be found here.\nNext, performing some basic quality control filtering. (Need to explain, justify, and elaborate on this step).\n\n\nShow Code\n# QC filtering choices\nSNR_THRESHOLD &lt;- 70\nMIN_TEFF_THRESHOLD &lt;- 3500\nMAX_TEFF_THRESHOLD &lt;- 5500\nLOGG_THRESHOLD &lt;- 3.6\nSTARFLAG &lt;- 0\nVB_THRESHOLD &lt;- 0.9\n\n# Apply QC filter\ndata_qc_filtered &lt;- apply_qc_filtering(\n  data_orig,\n  snr_threshold = SNR_THRESHOLD,\n  teff_thresholds = c(MIN_TEFF_THRESHOLD, MAX_TEFF_THRESHOLD),\n  logg_threshold = LOGG_THRESHOLD,\n  starflag = STARFLAG,\n  vb_threshold = VB_THRESHOLD\n)\n\n\nQuick overview of the data after QC filtering…\n\nAbundance DataMetadataGC Frequency Table\n\n\n\nShow Code\nabundance_df &lt;- data_qc_filtered$abundance\nskimr::skim(abundance_df)\n\n\nData summary\n\n\nName\nabundance_df\n\n\nNumber of rows\n3286\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n19\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nC_FE\n4\n1.00\n-0.17\n0.31\n-1.48\n-0.39\n-0.16\n0.04\n0.85\n▁▂▇▆▁\n\n\nCI_FE\n50\n0.98\n-0.07\n0.35\n-1.44\n-0.29\n-0.07\n0.14\n0.97\n▁▂▇▆▁\n\n\nN_FE\n3\n1.00\n0.61\n0.47\n-0.55\n0.27\n0.67\n0.94\n1.96\n▂▅▇▅▁\n\n\nO_FE\n18\n0.99\n0.28\n0.20\n-0.57\n0.18\n0.30\n0.40\n0.86\n▁▂▆▇▁\n\n\nNA_FE\n729\n0.78\n0.10\n0.51\n-1.61\n-0.19\n0.14\n0.43\n2.33\n▁▅▇▁▁\n\n\nMG_FE\n2\n1.00\n0.22\n0.17\n-0.50\n0.15\n0.25\n0.33\n0.81\n▁▂▇▇▁\n\n\nAL_FE\n20\n0.99\n0.25\n0.42\n-0.67\n-0.12\n0.24\n0.53\n1.38\n▃▆▇▃▂\n\n\nSI_FE\n2\n1.00\n0.26\n0.09\n-0.30\n0.21\n0.26\n0.31\n0.58\n▁▁▅▇▁\n\n\nS_FE\n106\n0.97\n0.40\n0.23\n-0.74\n0.30\n0.41\n0.53\n1.12\n▁▁▆▇▁\n\n\nK_FE\n189\n0.94\n0.24\n0.28\n-1.02\n0.14\n0.27\n0.38\n1.31\n▁▁▇▃▁\n\n\nCA_FE\n68\n0.98\n0.23\n0.18\n-0.76\n0.16\n0.24\n0.32\n0.90\n▁▁▇▇▁\n\n\nTI_FE\n302\n0.91\n0.03\n0.20\n-0.75\n-0.08\n0.03\n0.14\n0.96\n▁▃▇▁▁\n\n\nTIII_FE\n208\n0.94\n0.19\n0.25\n-0.78\n0.08\n0.22\n0.33\n1.02\n▁▁▇▅▁\n\n\nV_FE\n467\n0.86\n0.09\n0.41\n-1.32\n-0.15\n0.10\n0.34\n1.62\n▁▃▇▂▁\n\n\nCR_FE\n192\n0.94\n0.02\n0.37\n-1.07\n-0.20\n0.01\n0.22\n1.45\n▁▆▇▂▁\n\n\nMN_FE\n508\n0.85\n-0.28\n0.21\n-1.01\n-0.41\n-0.27\n-0.17\n0.81\n▁▇▇▁▁\n\n\nFE_H\n2\n1.00\n-1.44\n0.39\n-2.42\n-1.72\n-1.50\n-1.16\n0.33\n▁▇▃▁▁\n\n\nCO_FE\n286\n0.91\n-0.08\n0.44\n-1.42\n-0.42\n-0.07\n0.17\n1.52\n▁▅▇▂▁\n\n\nNI_FE\n60\n0.98\n0.00\n0.11\n-0.70\n-0.06\n0.00\n0.06\n0.62\n▁▁▇▁▁\n\n\n\n\n\n\nShow Code\nmetadata_df &lt;- data_qc_filtered$meta |&gt; \n  dplyr::mutate(\n    dplyr::across(where(is.character), as.factor)\n  )\nskimr::skim(metadata_df)\n\n\nData summary\n\n\nName\nmetadata_df\n\n\nNumber of rows\n3286\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nX_H\n0\n1.00\nFALSE\n3286\n(-0: 1, (-0: 1, (-0: 1, (-0: 1\n\n\nGC_NAME\n0\n1.00\nFALSE\n57\nNGC: 1221, NGC: 233, NGC: 228, NGC: 171\n\n\nAPSTAR_ID\n0\n1.00\nFALSE\n3286\napo: 1, apo: 1, apo: 1, apo: 1\n\n\nTELESCOPE\n0\n1.00\nFALSE\n2\nlco: 2829, apo: 457\n\n\nTARGFLAGS\n146\n0.96\nFALSE\n68\nAPO: 1141, APO: 959, APO: 224, APO: 171\n\n\nAPOGEE_ID\n0\n1.00\nFALSE\n3286\n2M0: 1, 2M0: 1, 2M0: 1, 2M0: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nGLON\n0\n1\n231.41\n130.85\n0.00\n59.20\n308.90\n309.17\n357.63\n▃▁▁▁▇\n\n\nGLAT\n0\n1\n4.10\n28.34\n-89.47\n-11.25\n14.84\n15.09\n79.90\n▁▃▅▇▁\n\n\nLOCATION_ID\n0\n1\n5438.53\n766.57\n2011.00\n5293.00\n5298.00\n5644.00\n7254.00\n▁▁▁▇▁\n\n\nSTARFLAG\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nVHELIO_AVG\n0\n1\n104.38\n158.94\n-257.40\n-23.62\n110.43\n231.11\n501.77\n▃▅▃▇▁\n\n\nTEFF_SPEC\n0\n1\n4571.09\n303.00\n3502.70\n4382.40\n4621.60\n4783.35\n5445.70\n▁▃▇▇▁\n\n\nLOGG_SPEC\n0\n1\n1.56\n0.56\n-0.19\n1.16\n1.59\n1.98\n3.20\n▁▅▇▆▁\n\n\nVB_PROB\n0\n1\n1.00\n0.00\n0.90\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nSNREV\n0\n1\n209.99\n143.20\n70.03\n119.26\n162.09\n249.67\n1310.13\n▇▁▁▁▁\n\n\n\n\n\n\nShow Code\ndata_qc_filtered$meta |&gt; \n  dplyr::group_by(GC_NAME) |&gt; \n  dplyr::summarise(\n    n = dplyr::n(),\n    .groups = \"drop\"\n  ) |&gt; \n  dplyr::arrange(dplyr::desc(n)) |&gt;\n  vthemes::pretty_DT()\n\n\n\n\n\n\nBefore proceeding any further, we will split data into training and test using 80-20% split.\n\n\nShow Code\n# Split data into training and test sets\nTRAIN_PROP &lt;- 0.8\n\ntrain_idxs &lt;- sample(\n  1:nrow(data_qc_filtered$abundance), \n  size = round(TRAIN_PROP * nrow(data_qc_filtered$abundance)),\n  replace = FALSE\n)\ntrain_data &lt;- data_qc_filtered$abundance[train_idxs, ]\ntest_data &lt;- data_qc_filtered$abundance[-train_idxs, ]\ntrain_metadata &lt;- data_qc_filtered$meta[train_idxs, ]\ntest_metadata &lt;- data_qc_filtered$meta[-train_idxs, ]\n\n\nNow imputing missing values and standardizing the data. We will try imputing using mean and random forest imputation.\n\n\nShow Code\n# impute missing values and standardize data\ndata_fpath &lt;- file.path(DATA_PATH, \"astro_cleaned_data.RData\")\nif (!file.exists(data_fpath)) {  # if not already cached\n  metadata &lt;- list(\n    train = train_metadata,\n    test = test_metadata\n  )\n  \n  data_mean_imputed &lt;- clean_astro_data(\n    train_data = train_data,\n    test_data = test_data,\n    impute_mode = \"mean\"\n  )\n  \n  data_rf_imputed &lt;- clean_astro_data(\n    train_data = train_data,\n    test_data = test_data,\n    impute_mode = \"rf\"\n  )\n  \n  save(\n    metadata, data_mean_imputed, data_rf_imputed, \n    file = file.path(DATA_PATH, \"astro_cleaned_data.RData\")\n  )\n} else {  # read from cache\n  load(data_fpath)\n}",
    "crumbs": [
      "2 Data"
    ]
  },
  {
    "objectID": "notebooks/01-astro-case-study-data.html#loading-cleaning-and-splitting-data",
    "href": "notebooks/01-astro-case-study-data.html#loading-cleaning-and-splitting-data",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Let us begin by loading in the APOGEE DR17 data, which was originally downloaded from SDSS (or direct download link here).\n\n\nShow Code\n# Load data\ndata_orig &lt;- load_astro_data(data_dir = DATA_PATH)\n\n\nThis data contains both the chemical abundance data and metadata about the quality control and star properties.\nA detailed data dictionary can be found here.\nNext, performing some basic quality control filtering. (Need to explain, justify, and elaborate on this step).\n\n\nShow Code\n# QC filtering choices\nSNR_THRESHOLD &lt;- 70\nMIN_TEFF_THRESHOLD &lt;- 3500\nMAX_TEFF_THRESHOLD &lt;- 5500\nLOGG_THRESHOLD &lt;- 3.6\nSTARFLAG &lt;- 0\nVB_THRESHOLD &lt;- 0.9\n\n# Apply QC filter\ndata_qc_filtered &lt;- apply_qc_filtering(\n  data_orig,\n  snr_threshold = SNR_THRESHOLD,\n  teff_thresholds = c(MIN_TEFF_THRESHOLD, MAX_TEFF_THRESHOLD),\n  logg_threshold = LOGG_THRESHOLD,\n  starflag = STARFLAG,\n  vb_threshold = VB_THRESHOLD\n)\n\n\nQuick overview of the data after QC filtering…\n\nAbundance DataMetadataGC Frequency Table\n\n\n\nShow Code\nabundance_df &lt;- data_qc_filtered$abundance\nskimr::skim(abundance_df)\n\n\nData summary\n\n\nName\nabundance_df\n\n\nNumber of rows\n3286\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n19\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nC_FE\n4\n1.00\n-0.17\n0.31\n-1.48\n-0.39\n-0.16\n0.04\n0.85\n▁▂▇▆▁\n\n\nCI_FE\n50\n0.98\n-0.07\n0.35\n-1.44\n-0.29\n-0.07\n0.14\n0.97\n▁▂▇▆▁\n\n\nN_FE\n3\n1.00\n0.61\n0.47\n-0.55\n0.27\n0.67\n0.94\n1.96\n▂▅▇▅▁\n\n\nO_FE\n18\n0.99\n0.28\n0.20\n-0.57\n0.18\n0.30\n0.40\n0.86\n▁▂▆▇▁\n\n\nNA_FE\n729\n0.78\n0.10\n0.51\n-1.61\n-0.19\n0.14\n0.43\n2.33\n▁▅▇▁▁\n\n\nMG_FE\n2\n1.00\n0.22\n0.17\n-0.50\n0.15\n0.25\n0.33\n0.81\n▁▂▇▇▁\n\n\nAL_FE\n20\n0.99\n0.25\n0.42\n-0.67\n-0.12\n0.24\n0.53\n1.38\n▃▆▇▃▂\n\n\nSI_FE\n2\n1.00\n0.26\n0.09\n-0.30\n0.21\n0.26\n0.31\n0.58\n▁▁▅▇▁\n\n\nS_FE\n106\n0.97\n0.40\n0.23\n-0.74\n0.30\n0.41\n0.53\n1.12\n▁▁▆▇▁\n\n\nK_FE\n189\n0.94\n0.24\n0.28\n-1.02\n0.14\n0.27\n0.38\n1.31\n▁▁▇▃▁\n\n\nCA_FE\n68\n0.98\n0.23\n0.18\n-0.76\n0.16\n0.24\n0.32\n0.90\n▁▁▇▇▁\n\n\nTI_FE\n302\n0.91\n0.03\n0.20\n-0.75\n-0.08\n0.03\n0.14\n0.96\n▁▃▇▁▁\n\n\nTIII_FE\n208\n0.94\n0.19\n0.25\n-0.78\n0.08\n0.22\n0.33\n1.02\n▁▁▇▅▁\n\n\nV_FE\n467\n0.86\n0.09\n0.41\n-1.32\n-0.15\n0.10\n0.34\n1.62\n▁▃▇▂▁\n\n\nCR_FE\n192\n0.94\n0.02\n0.37\n-1.07\n-0.20\n0.01\n0.22\n1.45\n▁▆▇▂▁\n\n\nMN_FE\n508\n0.85\n-0.28\n0.21\n-1.01\n-0.41\n-0.27\n-0.17\n0.81\n▁▇▇▁▁\n\n\nFE_H\n2\n1.00\n-1.44\n0.39\n-2.42\n-1.72\n-1.50\n-1.16\n0.33\n▁▇▃▁▁\n\n\nCO_FE\n286\n0.91\n-0.08\n0.44\n-1.42\n-0.42\n-0.07\n0.17\n1.52\n▁▅▇▂▁\n\n\nNI_FE\n60\n0.98\n0.00\n0.11\n-0.70\n-0.06\n0.00\n0.06\n0.62\n▁▁▇▁▁\n\n\n\n\n\n\nShow Code\nmetadata_df &lt;- data_qc_filtered$meta |&gt; \n  dplyr::mutate(\n    dplyr::across(where(is.character), as.factor)\n  )\nskimr::skim(metadata_df)\n\n\nData summary\n\n\nName\nmetadata_df\n\n\nNumber of rows\n3286\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nX_H\n0\n1.00\nFALSE\n3286\n(-0: 1, (-0: 1, (-0: 1, (-0: 1\n\n\nGC_NAME\n0\n1.00\nFALSE\n57\nNGC: 1221, NGC: 233, NGC: 228, NGC: 171\n\n\nAPSTAR_ID\n0\n1.00\nFALSE\n3286\napo: 1, apo: 1, apo: 1, apo: 1\n\n\nTELESCOPE\n0\n1.00\nFALSE\n2\nlco: 2829, apo: 457\n\n\nTARGFLAGS\n146\n0.96\nFALSE\n68\nAPO: 1141, APO: 959, APO: 224, APO: 171\n\n\nAPOGEE_ID\n0\n1.00\nFALSE\n3286\n2M0: 1, 2M0: 1, 2M0: 1, 2M0: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nGLON\n0\n1\n231.41\n130.85\n0.00\n59.20\n308.90\n309.17\n357.63\n▃▁▁▁▇\n\n\nGLAT\n0\n1\n4.10\n28.34\n-89.47\n-11.25\n14.84\n15.09\n79.90\n▁▃▅▇▁\n\n\nLOCATION_ID\n0\n1\n5438.53\n766.57\n2011.00\n5293.00\n5298.00\n5644.00\n7254.00\n▁▁▁▇▁\n\n\nSTARFLAG\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nVHELIO_AVG\n0\n1\n104.38\n158.94\n-257.40\n-23.62\n110.43\n231.11\n501.77\n▃▅▃▇▁\n\n\nTEFF_SPEC\n0\n1\n4571.09\n303.00\n3502.70\n4382.40\n4621.60\n4783.35\n5445.70\n▁▃▇▇▁\n\n\nLOGG_SPEC\n0\n1\n1.56\n0.56\n-0.19\n1.16\n1.59\n1.98\n3.20\n▁▅▇▆▁\n\n\nVB_PROB\n0\n1\n1.00\n0.00\n0.90\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nSNREV\n0\n1\n209.99\n143.20\n70.03\n119.26\n162.09\n249.67\n1310.13\n▇▁▁▁▁\n\n\n\n\n\n\nShow Code\ndata_qc_filtered$meta |&gt; \n  dplyr::group_by(GC_NAME) |&gt; \n  dplyr::summarise(\n    n = dplyr::n(),\n    .groups = \"drop\"\n  ) |&gt; \n  dplyr::arrange(dplyr::desc(n)) |&gt;\n  vthemes::pretty_DT()\n\n\n\n\n\n\nBefore proceeding any further, we will split data into training and test using 80-20% split.\n\n\nShow Code\n# Split data into training and test sets\nTRAIN_PROP &lt;- 0.8\n\ntrain_idxs &lt;- sample(\n  1:nrow(data_qc_filtered$abundance), \n  size = round(TRAIN_PROP * nrow(data_qc_filtered$abundance)),\n  replace = FALSE\n)\ntrain_data &lt;- data_qc_filtered$abundance[train_idxs, ]\ntest_data &lt;- data_qc_filtered$abundance[-train_idxs, ]\ntrain_metadata &lt;- data_qc_filtered$meta[train_idxs, ]\ntest_metadata &lt;- data_qc_filtered$meta[-train_idxs, ]\n\n\nNow imputing missing values and standardizing the data. We will try imputing using mean and random forest imputation.\n\n\nShow Code\n# impute missing values and standardize data\ndata_fpath &lt;- file.path(DATA_PATH, \"astro_cleaned_data.RData\")\nif (!file.exists(data_fpath)) {  # if not already cached\n  metadata &lt;- list(\n    train = train_metadata,\n    test = test_metadata\n  )\n  \n  data_mean_imputed &lt;- clean_astro_data(\n    train_data = train_data,\n    test_data = test_data,\n    impute_mode = \"mean\"\n  )\n  \n  data_rf_imputed &lt;- clean_astro_data(\n    train_data = train_data,\n    test_data = test_data,\n    impute_mode = \"rf\"\n  )\n  \n  save(\n    metadata, data_mean_imputed, data_rf_imputed, \n    file = file.path(DATA_PATH, \"astro_cleaned_data.RData\")\n  )\n} else {  # read from cache\n  load(data_fpath)\n}",
    "crumbs": [
      "2 Data"
    ]
  },
  {
    "objectID": "notebooks/03-astro-case-study-dimension-reduction.html",
    "href": "notebooks/03-astro-case-study-dimension-reduction.html",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Fit Dimension Reduction Methods\n## this code chunk fits the dimension reduction methods\n\n# select dimension reduction hyperparameter grids\nTSNE_PERPLEXITIES &lt;- c(10, 30, 60, 100, 300)\nUMAP_N_NEIGHBORS &lt;- c(10, 30, 60, 100, 300)\n\n# select dimension reduction methods\ndr_fun_ls &lt;- c(\n  list(\"PCA\" = fit_pca),\n  purrr::map(\n    TSNE_PERPLEXITIES,\n    ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n  ) |&gt; \n    setNames(sprintf(\"tSNE (perplexity = %d)\", TSNE_PERPLEXITIES)),\n  purrr::map(\n    UMAP_N_NEIGHBORS,\n    ~ purrr::partial(fit_umap, dims = 2, n_neighbors = .x\n    )\n  ) |&gt; \n    setNames(sprintf(\"UMAP (n_neighbors = %d)\", UMAP_N_NEIGHBORS))\n)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"dimension_reduction_fits.rds\")\nif (!file.exists(fit_results_fname)) {\n  # fit dimension reduction methods (if not already cached)\n  dr_fit_ls &lt;- purrr::map(\n    train_data_ls,\n    function(train_data) {\n      purrr::map(dr_fun_ls, function(dr_fun) dr_fun(train_data))\n    }\n  )\n  # save dimension reduction fits\n  saveRDS(dr_fit_ls, file = fit_results_fname)\n} else {\n  # read in dimension reduction fits (if already cached)\n  dr_fit_ls &lt;- readRDS(fit_results_fname)\n}\n\n# aggregate all dimension reduction results into one df\nplt_df &lt;- purrr::list_flatten(dr_fit_ls, name_spec = \"{inner} [{outer}]\") |&gt; \n  purrr::map(\n    ~ .x$scores[, 1:2] |&gt; \n      setNames(sprintf(\"Component %d\", 1:2)) |&gt; \n      dplyr::bind_cols(\n        metadata$train |&gt; dplyr::select(GC_NAME, GLAT, GLON)\n      ) |&gt; \n      dplyr::mutate(\n        id = 1:dplyr::n()\n      )\n  ) |&gt; \n  dr_results_to_df()\n\n\n\n\nShow Code to Tune/Evaluate Dimension Reduction Methods\n## this code chunk evaluates the dimension reduction methods\n\n# evaluate neighborhood retention metric\nKs &lt;- c(1, 5, 10, 25, 50, 100, 200, 300)\n\neval_results_fname &lt;- file.path(RESULTS_PATH, \"dimension_reduction_eval.rds\")\nif (!file.exists(eval_results_fname)) {\n  # evaluate neighborhood retention (if not already cached)\n  dr_eval_ls &lt;- purrr::imap(\n    dr_fit_ls,\n    function(dr_out, key) {\n      purrr::map(\n        dr_out, \n        function(.x) {\n          eval_neighborhood_retention(\n            orig_data = train_data_ls[[key]],\n            dr_data = .x$scores[, 1:min(ncol(.x$scores), 4)],\n            ks = Ks\n          )\n        }\n      )\n    }\n  )\n  # save dimension reduction evaluation results\n  saveRDS(dr_eval_ls, file = eval_results_fname)\n} else {\n  # read in dimension reduction evaluation results (if already cached)\n  dr_eval_ls &lt;- readRDS(eval_results_fname)\n}\n\neval_plt_df &lt;- purrr::list_flatten(\n  dr_eval_ls, name_spec = \"{inner} [{outer}]\"\n) |&gt; \n  dr_results_to_df()\n\n\n\n\nUMAP always below tSNE (100)\n\n\n\n\n\n\nBy GCGalatic Coordinates by Component 1Galatic Coordinates by Component 2PC Directions\n\n\n\n\n\n\nDimension reduction visualizations, colored by GC.\n\n\n\n\n\n\n\n\n\nGalactic coordinates plot (jittered), colored by value of the first component from dimension reduction method.\n\n\n\n\n\n\n\n\n\n\n\nGalactic coordinates plot (jittered), colored by value of the second component from dimension reduction method.\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Loadings",
    "crumbs": [
      "4 Dimension Reduction"
    ]
  },
  {
    "objectID": "notebooks/03-astro-case-study-dimension-reduction.html#dimension-reduction",
    "href": "notebooks/03-astro-case-study-dimension-reduction.html#dimension-reduction",
    "title": "Finding Common Origins of Milky Way Stars",
    "section": "",
    "text": "Show Code to Fit Dimension Reduction Methods\n## this code chunk fits the dimension reduction methods\n\n# select dimension reduction hyperparameter grids\nTSNE_PERPLEXITIES &lt;- c(10, 30, 60, 100, 300)\nUMAP_N_NEIGHBORS &lt;- c(10, 30, 60, 100, 300)\n\n# select dimension reduction methods\ndr_fun_ls &lt;- c(\n  list(\"PCA\" = fit_pca),\n  purrr::map(\n    TSNE_PERPLEXITIES,\n    ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)\n  ) |&gt; \n    setNames(sprintf(\"tSNE (perplexity = %d)\", TSNE_PERPLEXITIES)),\n  purrr::map(\n    UMAP_N_NEIGHBORS,\n    ~ purrr::partial(fit_umap, dims = 2, n_neighbors = .x\n    )\n  ) |&gt; \n    setNames(sprintf(\"UMAP (n_neighbors = %d)\", UMAP_N_NEIGHBORS))\n)\n\nfit_results_fname &lt;- file.path(RESULTS_PATH, \"dimension_reduction_fits.rds\")\nif (!file.exists(fit_results_fname)) {\n  # fit dimension reduction methods (if not already cached)\n  dr_fit_ls &lt;- purrr::map(\n    train_data_ls,\n    function(train_data) {\n      purrr::map(dr_fun_ls, function(dr_fun) dr_fun(train_data))\n    }\n  )\n  # save dimension reduction fits\n  saveRDS(dr_fit_ls, file = fit_results_fname)\n} else {\n  # read in dimension reduction fits (if already cached)\n  dr_fit_ls &lt;- readRDS(fit_results_fname)\n}\n\n# aggregate all dimension reduction results into one df\nplt_df &lt;- purrr::list_flatten(dr_fit_ls, name_spec = \"{inner} [{outer}]\") |&gt; \n  purrr::map(\n    ~ .x$scores[, 1:2] |&gt; \n      setNames(sprintf(\"Component %d\", 1:2)) |&gt; \n      dplyr::bind_cols(\n        metadata$train |&gt; dplyr::select(GC_NAME, GLAT, GLON)\n      ) |&gt; \n      dplyr::mutate(\n        id = 1:dplyr::n()\n      )\n  ) |&gt; \n  dr_results_to_df()\n\n\n\n\nShow Code to Tune/Evaluate Dimension Reduction Methods\n## this code chunk evaluates the dimension reduction methods\n\n# evaluate neighborhood retention metric\nKs &lt;- c(1, 5, 10, 25, 50, 100, 200, 300)\n\neval_results_fname &lt;- file.path(RESULTS_PATH, \"dimension_reduction_eval.rds\")\nif (!file.exists(eval_results_fname)) {\n  # evaluate neighborhood retention (if not already cached)\n  dr_eval_ls &lt;- purrr::imap(\n    dr_fit_ls,\n    function(dr_out, key) {\n      purrr::map(\n        dr_out, \n        function(.x) {\n          eval_neighborhood_retention(\n            orig_data = train_data_ls[[key]],\n            dr_data = .x$scores[, 1:min(ncol(.x$scores), 4)],\n            ks = Ks\n          )\n        }\n      )\n    }\n  )\n  # save dimension reduction evaluation results\n  saveRDS(dr_eval_ls, file = eval_results_fname)\n} else {\n  # read in dimension reduction evaluation results (if already cached)\n  dr_eval_ls &lt;- readRDS(eval_results_fname)\n}\n\neval_plt_df &lt;- purrr::list_flatten(\n  dr_eval_ls, name_spec = \"{inner} [{outer}]\"\n) |&gt; \n  dr_results_to_df()\n\n\n\n\nUMAP always below tSNE (100)\n\n\n\n\n\n\nBy GCGalatic Coordinates by Component 1Galatic Coordinates by Component 2PC Directions\n\n\n\n\n\n\nDimension reduction visualizations, colored by GC.\n\n\n\n\n\n\n\n\n\nGalactic coordinates plot (jittered), colored by value of the first component from dimension reduction method.\n\n\n\n\n\n\n\n\n\n\n\nGalactic coordinates plot (jittered), colored by value of the second component from dimension reduction method.\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Loadings",
    "crumbs": [
      "4 Dimension Reduction"
    ]
  }
]