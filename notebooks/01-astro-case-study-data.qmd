---
title: "Finding Common Origins of Milky Way Stars"
date: today
author: "Andersen Chang$^*$, Tiffany M. Tang$^*$, Tarek M. Zikry$^*$, Genevera I. Allen"
format:
  html:
    code-fold: show
---

```{r}
#| include: false

rm(list = ls())

# source all functions from R/ folder
for (fname in list.files(here::here("R"), pattern = "*.R")) {
  source(here::here(file.path("R", fname)))
}

# helper paths
DATA_PATH <- here::here("data")
RESULTS_PATH <- here::here("results")
if (!file.exists(RESULTS_PATH)) {
  dir.create(RESULTS_PATH, recursive = TRUE)
}

# helpers for quarto plotting
subchunkify <- purrr::partial(subchunkify, prefix = "data")
subchunk_idx <- 1  # for subchunk numbering

set.seed(12345)
```

## Loading, Cleaning, and Splitting Data

Let us begin by loading in the APOGEE DR17 data, which was originally downloaded from [SDSS](https://www.sdss4.org/dr17/irspec/spectro_data/) (or direct download link [here](https://data.sdss.org/sas/dr17/apogee/spectro/aspcap/dr17/synspec_rev1/allStar-dr17-synspec_rev1.fits)).

```{r}
# Load data
data_orig <- load_astro_data(data_dir = DATA_PATH)
```

This data contains both the chemical abundance data and metadata about the quality control and star properties.

A detailed data dictionary can be found [here](https://data.sdss.org/datamodel/files/APOGEE_ASPCAP/APRED_VERS/ASPCAP_VERS/allStar.html).

Next, performing some basic quality control filtering. (Need to explain, justify, and elaborate on this step).

```{r}
# QC filtering choices
SNR_THRESHOLD <- 70
MIN_TEFF_THRESHOLD <- 3500
MAX_TEFF_THRESHOLD <- 5500
LOGG_THRESHOLD <- 3.6
STARFLAG <- 0
VB_THRESHOLD <- 0.9

# Apply QC filter
data_qc_filtered <- apply_qc_filtering(
  data_orig,
  snr_threshold = SNR_THRESHOLD,
  teff_thresholds = c(MIN_TEFF_THRESHOLD, MAX_TEFF_THRESHOLD),
  logg_threshold = LOGG_THRESHOLD,
  starflag = STARFLAG,
  vb_threshold = VB_THRESHOLD
)
```

Quick overview of the data after QC filtering...

:::{.panel-tabset}

### Abundance Data

```{r}
#| output: asis
#| code-fold: true

abundance_df <- data_qc_filtered$abundance
skimr::skim(abundance_df)
```

### Metadata

```{r}
#| output: asis
#| code-fold: true

metadata_df <- data_qc_filtered$meta |> 
  dplyr::mutate(
    dplyr::across(where(is.character), as.factor)
  )
skimr::skim(metadata_df)
```

### GC Frequency Table

```{r}
#| output: asis
#| code-fold: true

data_qc_filtered$meta |> 
  dplyr::group_by(GC_NAME) |> 
  dplyr::summarise(
    n = dplyr::n(),
    .groups = "drop"
  ) |> 
  dplyr::arrange(dplyr::desc(n)) |>
  vthemes::pretty_DT()
```

:::

Before proceeding any further, we will split data into training and test using 80-20% split.

```{r}
# Split data into training and test sets
TRAIN_PROP <- 0.8

train_idxs <- sample(
  1:nrow(data_qc_filtered$abundance), 
  size = round(TRAIN_PROP * nrow(data_qc_filtered$abundance)),
  replace = FALSE
)
train_data <- data_qc_filtered$abundance[train_idxs, ]
test_data <- data_qc_filtered$abundance[-train_idxs, ]
train_metadata <- data_qc_filtered$meta[train_idxs, ]
test_metadata <- data_qc_filtered$meta[-train_idxs, ]
```

Now imputing missing values and standardizing the data. We will try imputing using mean and random forest imputation.

```{r}
# impute missing values and standardize data
data_fpath <- file.path(DATA_PATH, "astro_cleaned_data.RData")
if (!file.exists(data_fpath)) {  # if not already cached
  metadata <- list(
    train = train_metadata,
    test = test_metadata
  )
  
  data_mean_imputed <- clean_astro_data(
    train_data = train_data,
    test_data = test_data,
    impute_mode = "mean"
  )
  
  data_rf_imputed <- clean_astro_data(
    train_data = train_data,
    test_data = test_data,
    impute_mode = "rf"
  )
  
  save(
    metadata, data_mean_imputed, data_rf_imputed, 
    file = file.path(DATA_PATH, "astro_cleaned_data.RData")
  )
} else {  # read from cache
  load(data_fpath)
}
```
