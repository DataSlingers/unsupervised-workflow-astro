---
title: "Finding Common Origins of Milky Way Stars"
date: today
author: "Andersen Chang, Tiffany M. Tang, Tarek M. Zikry, Genevera I. Allen"
format: html
---

```{r setup}
#| include: false

rm(list = ls())

# source all functions from R/ folder
for (fname in list.files(here::here("R"), pattern = "*.R")) {
  source(here::here(file.path("R", fname)))
}

# helper paths
DATA_PATH <- here::here("data")
RESULTS_PATH <- here::here("results")
if (!file.exists(RESULTS_PATH)) {
  dir.create(RESULTS_PATH, recursive = TRUE)
}

# helpers for quarto plotting
subchunkify <- purrr::partial(subchunkify, prefix = "train")
subchunk_idx <- 1  # for subchunk numbering

# load in data
load(file.path(DATA_PATH, "astro_cleaned_data.RData"))
# load in dimension reduction results
dr_fit_ls <- readRDS(file.path(RESULTS_PATH, "dimension_reduction_fits.rds"))

set.seed(12345)

custom_colors <- c("black", "#d9d9d9", "white", "#fadbd5", "#BD7A65")
custom_color_values <- c(0, 0.3, 0.5, 0.7, 1)

NCORES <- as.integer(Sys.getenv("NCORES"))
if (is.na(NCORES)) {
  NCORES <- parallel::detectCores() - 1
}
```

## Fit Clustering Pipelines on Training Data

In this section, we detail our clustering pipeline.

**Clustering Methods Under Study:**

- K-means Clustering
- Hierarchical Clustering with Euclidean distance and either complete or Ward's linkage
- Spectral Clustering with nearest neighbors affinity and number of neighbors = 5, 30, 60, or 100

For each of these clustering methods, we examine the clustering results across different choices of the number of clusters $k = 2, \ldots, 30$.

There are also many different ways one could apply these clustering methods. 
For example, one could apply it to the chemical abundance data directly or to a dimension-reduced version of the chemical abundance data. 
There are also other preprocessing choices, such as how to perform quality control filtering, which chemical abundance features are used, and how to impute missing data.
In the figure below, we outline the various data preprocessing pipelines (prior to clustering) that are considered in this case study.
Note that we only included the (tuned) dimension reduction methods that exhibited high neighborhood retention from the previous investigation (i.e., PCA, tSNE (perplexity = 30), and tSNE (perplexity = 100)), and for PCA, the top four components were chosen based on the scree plot.
We will refer to the set of these data preprocessing pipelines as $\mathcal{G}$.

![Set of data preprocessing pipelines considered prior to applying clustering methods in case study. Pipelines marked with an astericks were excluded to reduce the computational burden; however, results are similar even when including these pipelines.](../figures/data_preprocessing_pipeline.jpg){width="75%"}

```{r}
#| code-summary: "Show Code to Fit Clustering Pipelines on Training Data"

## this code chunk fits the clustering pipelines on the training data

#### choose imputation methods ####
data_ls <- list(
  "Mean-imputed" = data_mean_imputed$train,
  "RF-imputed" = data_rf_imputed$train
)

#### choose number of features ####
feature_modes <- list(
  "Small" = 7,
  "Medium" = 11,
  "Big" = 19
)

#### choose ks ####
ks <- 2:30

#### choose dimension reduction methods ####
# raw data
identity_fun_ls <- list("Raw" = function(x) x)

# pca
pca_fun_ls <- list("PCA" = purrr::partial(fit_pca, ndim = 4))

# tsne
tsne_perplexities <- c(30, 100)
tsne_fun_ls <- purrr::map(
  tsne_perplexities,
  ~ purrr::partial(fit_tsne, dims = 2, perplexity = .x)
) |> 
  setNames(sprintf("tSNE (perplexity = %d)", tsne_perplexities))

# putting it together
dr_fun_ls <- c(
  identity_fun_ls,
  pca_fun_ls,
  tsne_fun_ls
)

#### choose clustering methods ####
# kmeans
kmeans_fun_ls <- list("K-means" = purrr::partial(fit_kmeans, ks = ks))

# hierarchical clustering
hclust_params <- list(
  d = c("euclidean"),
  linkage = c("complete", "ward.D")
) |> 
  expand.grid()
hclust_fun_ls <- purrr::map(
  1:nrow(hclust_params),
  ~ purrr::partial(
    fit_hclust, 
    ks = ks,
    d = hclust_params$d[[.x]],
    linkage = hclust_params$linkage[[.x]]
  )
) |> 
  setNames(
    sprintf(
      "Hierarchical (dist = %s, link = %s)",
      hclust_params$d, hclust_params$linkage
    )
  )

# spectral clustering
n_neighbors <- c(5, 30, 60, 100)
spectral_fun_ls <- purrr::map(
  n_neighbors,
  ~ purrr::partial(
    fit_spectral_clustering, 
    ks = ks,
    affinity = "nearest_neighbors",
    n_neighbors = .x
  )
) |> 
  setNames(sprintf("Spectral (n_neighbors = %s)", n_neighbors))

# putting it together
clust_fun_ls <- c(
  kmeans_fun_ls,
  hclust_fun_ls,
  spectral_fun_ls
)

#### Fit Clustering Pipelines ####
pipe_tib <- tidyr::expand_grid(
  data = data_ls,
  feature_mode = feature_modes,
  dr_method = dr_fun_ls,
  clust_method = clust_fun_ls
) |> 
  dplyr::mutate(
    impute_mode_name = names(data),
    feature_mode_name = names(feature_mode),
    dr_method_name = names(dr_method),
    clust_method_name = names(clust_method),
    name = stringr::str_glue(
      "{clust_method_name} [{impute_mode_name} + {feature_mode_name} + {dr_method_name}]"
    )
  ) |> 
  # remove some clustering pipelines to reduce computation burden
  dplyr::filter(
    # remove all big feature set + dimension-reduction runs
    !((dr_method_name != "Raw") & (feature_mode_name == "Big"))
  )
pipe_ls <- split(pipe_tib, seq_len(nrow(pipe_tib))) |> 
  setNames(pipe_tib$name)

fit_results_fname <- file.path(RESULTS_PATH, "clustering_fits_train.rds")
if (!file.exists(fit_results_fname)) {
  library(future)
  plan(multisession, workers = NCORES)
  
  # fit clustering pipelines (if not already cached)
  clust_fit_ls <- furrr::future_map(
    pipe_ls,
    function(pipe_df) {
      g <- create_preprocessing_pipeline(
        feature_mode = pipe_df$feature_mode[[1]],
        preprocess_fun = pipe_df$dr_method[[1]]
      )
      clust_out <- pipe_df$clust_method[[1]](
        data = pipe_df$data[[1]], preprocess_fun = g
      )
      return(clust_out)
    },
    .options = furrr::furrr_options(
      seed = TRUE, 
      globals = list(
        ks = ks,
        create_preprocessing_pipeline = create_preprocessing_pipeline,
        get_abundance_data = get_abundance_data,
        tsne_perplexities = tsne_perplexities,
        hclust_params = hclust_params,
        n_neighbors = n_neighbors,
        fit_kmeans = fit_kmeans,
        fit_hclust = fit_hclust,
        fit_spectral_clustering = fit_spectral_clustering
      )
    )
  )
  # save fitted clustering pipelines
  saveRDS(clust_fit_ls, file = fit_results_fname)
} else {
  # read in fitted clustering pipelines (if already cached)
  clust_fit_ls <- readRDS(fit_results_fname)
}
```

To see and interact with the training clustering results, we have developed a shiny app, which can be run locally in R/RStudio by typing the following in your R console:

```{r}
#| eval: false
#| code-fold: false

view_clusters_app()
```

## Hyperparameter Tuning and Model Selection via Cluster Stability

To select the best clustering method and number of clusters $k$, we propose a stability-driven model selection procedure.
Here, given our goal of working towards scientific discovery, we operate under the driving philosophy that our scientific conclusions should not be robust or stable across arbitrary human judgment calls such as the data preprocessing pipelines outlined in the above figure.
Driven by this "stability principle", we hence select the clustering method and number of clusters that yields the most stable (or similar) clustering results regardless of the data subsample used for training and regardless of the choice of data preprocessing pipeline $g \in \mathcal{G}$.
To operationalize this idea, we implement Algorithm 1 below, which is inspired by the Model Explorer algorithm from @ben2001stability.

![](../figures/algorithm1.png){width="75%"}

```{r}
#| code-summary: "Show Code to Tune/Evaluate Clustering Pipelines via Cluster Stability"

## this code chunk performs model selection and hyperparameter tuning for the trained clustering pipelines

#### ModelExplorer hyperparameters ####
n_subsamples <- 50 # Number of subsamples
subsample_frac <- 0.8 # Proportion of data to subsample
max_pairs <- 100  # max num of pairs of subsamples to evaluate cluster stability
metric <- "ARI"  # metrics to compute for cluster stability

n <- nrow(data_ls$`Mean-imputed`)
fit_results_fname <- file.path(RESULTS_PATH, "clustering_subsampled_fits.rds")
stability_results_fname <- file.path(
  RESULTS_PATH, sprintf("clustering_subsampled_stability_%s.RData", metric)
)
if (!file.exists(fit_results_fname) ||
    !file.exists(stability_results_fname)) {
  library(future)
  plan(multisession, workers = NCORES)

  # fit clustering pipelines on subsampled data (if not already cached)
  clust_subsampled_fit_ls <- purrr::map(
    1:n_subsamples,
    function(b) {
      # get subsampled indices
      subsampled_idxs <- sort(
        sample(1:n, size = subsample_frac * n, replace = FALSE)
      )
      furrr::future_map(
        pipe_ls,
        function(pipe_df) {
          # fit clustering pipeline on subsample
          g <- create_preprocessing_pipeline(
            feature_mode = pipe_df$feature_mode[[1]],
            preprocess_fun = pipe_df$dr_method[[1]]
          )
          clust_out <- pipe_df$clust_method[[1]](
            data = pipe_df$data[[1]][subsampled_idxs, ], preprocess_fun = g
          )
          # return only the cluster ids for subsample
          cluster_ids <- purrr::map(
            clust_out$cluster_ids,
            function(cluster_ids) {
              out <- rep(NA, nrow(pipe_df$data[[1]]))
              out[subsampled_idxs] <- cluster_ids
              return(out)
            }
          )
          return(cluster_ids)
        },
        .options = furrr::furrr_options(
          seed = TRUE, 
          globals = list(
            ks = ks,
            subsampled_idxs = subsampled_idxs,
            create_preprocessing_pipeline = create_preprocessing_pipeline,
            get_abundance_data = get_abundance_data,
            tsne_perplexities = tsne_perplexities,
            hclust_params = hclust_params,
            n_neighbors = n_neighbors,
            fit_kmeans = fit_kmeans,
            fit_hclust = fit_hclust,
            fit_spectral_clustering = fit_spectral_clustering
          )
        )
      )
    }
  ) |> 
    setNames(as.character(1:n_subsamples)) |> 
    purrr::list_flatten(name_spec = "{inner} {outer}") |> 
    purrr::list_flatten(name_spec = "{inner}: {outer}")
  # save fitted clustering pipelines on subsampled data
  saveRDS(clust_subsampled_fit_ls, file = fit_results_fname)
  
  # create tibble of clustering results
  clusters_tib <- tibble::tibble(
    name = names(clust_subsampled_fit_ls),
    cluster_ids = clust_subsampled_fit_ls
  ) |> 
    annotate_clustering_results()
  
  # evaluate stability of clustering results per pipeline
  stability_per_pipeline <- clusters_tib |> 
    dplyr::group_by(
      k, pipeline_name, 
      impute_mode_name, feature_mode_name, dr_method_name, clust_method_name
    ) |>
    dplyr::summarise(
      cluster_list = list(cluster_ids),
      .groups = "drop"
    )
  stability_vals <- furrr::future_map(
    stability_per_pipeline$cluster_list,
    function(clusters) {
      cluster_stability(
        clusters, max_pairs = max_pairs, metrics = metric
      )[[metric]]
    },
    .options = furrr::furrr_options(
      seed = TRUE, 
      globals = list(
        cluster_stability = cluster_stability,
        max_pairs = max_pairs,
        metric = metric
      )
    )
  )
  stability_per_pipeline <- stability_per_pipeline |> 
    dplyr::mutate(
      group_name = pipeline_name,
      stability = !!stability_vals
    )
  
  # evaluate stability of clustering results per clustering method
  stability_per_clust_method <- clusters_tib |> 
    dplyr::group_by(
      k, clust_method_name
    ) |>
    dplyr::summarise(
      pipeline_names = list(unique(pipeline_name)),
      cluster_list = list(cluster_ids),
      .groups = "drop"
    )
  stability_vals <- furrr::future_map2(
    stability_per_clust_method$cluster_list,
    stability_per_clust_method$pipeline_names,
    function(clusters, pipe_names) {
      cluster_stability(
        clusters, max_pairs = max_pairs * length(pipe_names), metrics = metric
      )[[metric]]
    },
    .options = furrr::furrr_options(
      seed = TRUE, 
      globals = list(
        cluster_stability = cluster_stability,
        max_pairs = max_pairs,
        metric = metric
      )
    )
  )
  stability_per_clust_method <- stability_per_clust_method |>
    dplyr::mutate(
      group_name = clust_method_name,
      stability = !!stability_vals
    )
  
  save(
    stability_per_pipeline, 
    stability_per_clust_method, 
    file = stability_results_fname
  )
} else {
  # read in fitted clustering pipelines on subsampled data (if already cached)
  clust_subsampled_fit_ls <- readRDS(fit_results_fname)
  # read in evaluated stability of clustering results
  load(stability_results_fname)
}
```

The main results from our stability-driven model selection procedure are shown in the figure below under the `Per Clustering Method` tab.

**Main Takeaways:**

- Spectral clustering (n\_neighbors = 60) with $k = 2$ clusters yields the most stable clusters
- Interestingly, K-means with $k = 8$ clusters also appears quite stable, considering that it is producing far more granular clusters than $k = 2$.

*Note: For completeness, the stability across subsamples per data preprocessing + clustering pipeline is provided under the `Per Pipeline` tab.*

:::{.panel-tabset .column-page-right}

```{r}
#| output: asis
#| echo: false

# plot stability results
stability_results_ls <- list(
  "Per Clustering Method" = stability_per_clust_method,
  "Per Pipeline" = stability_per_pipeline
)
for (stability_mode in names(stability_results_ls)) {
  cat(sprintf("\n\n### %s\n\n", stability_mode))
  plt_df <- stability_results_ls[[stability_mode]] |> 
    dplyr::mutate(
      mean_stability = purrr::map_dbl(stability, mean),
      sd_stability = purrr::map_dbl(stability, sd),
      n_stability = purrr::map_dbl(stability, length)
    )
  plt <- plt_df |> 
    ggplot2::ggplot() +
    ggplot2::aes(
      x = k, 
      y = mean_stability, 
      color = group_name
    ) +
    ggplot2::geom_line() +
    ggplot2::labs(x = "k", y = "Mean Stability", color = "") +
    vthemes::theme_vmodern()
  subchunkify(
    plotly::ggplotly(plt),
    caption = "'Stability of clustering pipelines across the number of clusters, measured via the adjusted Rand Index (ARI).'"
  )
}
```

:::

## Estimate Consensus Clusters

For both Spectral Clustering (n\_neighbors = 60) with $k = 2$ clusters and K-means with $k = 8$ clusters, we next aggregate the learned clusters across each of the data preprocessing pipelines using consensus clustering.

```{r}
#| code-summary: "Show Code to Estimate Consensus Clusters"

best_clust_methods <- list(
  "k = 2: Spectral (n_neighbors = 60)" = list(
    clust_method_name = "Spectral (n_neighbors = 60)",
    k = 2
  ),
  "k = 9: Spectral (n_neighbors = 60)" = list(
    clust_method_name = "Spectral (n_neighbors = 60)",
    k = 9
  ),
  "k = 8: K-means" = list(
    clust_method_name = "K-means", 
    k = 8
  )
)
clust_fit_ls <- purrr::map(clust_fit_ls, ~ .x$cluster_ids) |> 
  purrr::list_flatten(name_spec = "{inner}: {outer}")

consensus_clusters_results_path <- file.path(
  RESULTS_PATH, "consensus_clusters_train.rds"
)
consensus_nbhd_results_path <- file.path(
  RESULTS_PATH, "consensus_neighborhood_matrices_train.rds"
)
if (!file.exists(consensus_clusters_results_path) | 
    !file.exists(consensus_nbhd_results_path)) {
  nbhd_mat_ls <- list()
  consensus_out_ls <- list()
  for (i in 1:length(best_clust_methods)) {
    clust_method_name <- best_clust_methods[[i]]$clust_method_name
    k <- best_clust_methods[[i]]$k
    key <- names(best_clust_methods)[i]
    
    stable_clusters_tib_all <- tibble::tibble(
      name = names(clust_fit_ls),
      cluster_ids = clust_fit_ls
    ) |> 
      annotate_clustering_results() |> 
      dplyr::filter(
        k == !!k,
        clust_method_name == !!clust_method_name
      )
  
    keep_clust_ls <- stable_clusters_tib_all$cluster_ids
    # compute neighborhood matrix
    nbhd_mat_ls[[key]] <- get_consensus_neighborhood_matrix(keep_clust_ls)
    # aggregate stable clusters using consensus clustering
    consensus_out_ls[[key]] <- fit_consensus_clusters(nbhd_mat_ls[[key]], k = k)["cluster_ids"]
  }
  saveRDS(consensus_out_ls, file = consensus_clusters_results_path)
  saveRDS(nbhd_mat_ls, file = consensus_nbhd_results_path)
} else {
  # read in fitted clustering pipelines (if already cached)
  consensus_out_ls <- readRDS(consensus_clusters_results_path)
  nbhd_mat_ls <- readRDS(consensus_nbhd_results_path)
}
```

We visualize the resulting training consensus clusters in various ways below. Note:

- The frequencies shown in the consensus neighborhood heatmaps reveal the proportion of times that two stars were assigned to the same cluster across the different choices of data preprocessing pipelines.
    - High frequencies close to 1 indicate that the two stars were almost always in the same cluster. 
    - Low frequencies close to 0 in the consensus neighborhood heatmaps indicate that the two stars were almost never in the same cluster.
- For each star, we define its local stability to be the average co-clustering frequency (i.e., the value shown in the consensus neighborhood heatmap) with all other stars in the same cluster. Intuitively, the local stability score for a star captures how consistently the star is clustered with other stars in its estimated cluster. Low local stability suggests that the star is not consistently clustered and may "hop" from one cluster to another depending on the data preprocessing choices.

Clearly, we observe that the two clusters from Spectral Clustering (n\_neighbors = 60) are extremely stable; however, these two clusters largely recapitulate a well-known iron-rich versus iron-poor dichotomy between younger versus older stars in the Milky Way. Consequently, in an effort to seek new insights beyond this well-known phenomenon, we will turn our attention to the 8 clusters from K-means clustering for the remainder of this analysis.

:::{.panel-tabset}

```{r}
#| output: asis
#| echo: false

keep_dr_methods <- c("PCA", "tSNE (perplexity = 30)", "tSNE (perplexity = 100)")
keep_dr_feature_modes <- c("small", "medium")
dr_plt_df <- dr_fit_ls[keep_dr_feature_modes] |> 
  purrr::map(~ .x[keep_dr_methods]) |> 
  purrr::list_flatten(name_spec = "{inner} [{outer}]") |>
  purrr::map(
    ~ .x$scores[, 1:2] |> 
      setNames(paste0("Component ", 1:2)) |> 
      dplyr::mutate(
        id = 1:dplyr::n()
      )
  ) |> 
  dplyr::bind_rows(.id = "method") |> 
  dplyr::mutate(
    method = forcats::fct_inorder(method)
  )

for (key in c("k = 2: Spectral (n_neighbors = 60)", "k = 8: K-means")) {
  cat(sprintf("\n\n### %s\n\n", key))
  cat("\n\n:::{.panel-tabset .nav-pills}\n\n")
  clust_method_key <- stringr::str_remove(key, ".*:\\s")
  k_key <- stringr::str_extract(key, "(?<=k = )\\d+")
  nbhd_mat <- nbhd_mat_ls[[key]]
  consensus_cluster_ids <- consensus_out_ls[[key]]$cluster_ids
  
  cat("\n\n#### Overview\n\n")
  cat("\n\n##### Consensus Neighborhood Heatmap\n\n")
  consensus_plt <- plot_consensus_clusters(
    nbhd_mat,
    leaf_colors = as.factor(consensus_cluster_ids),
    custom_colors = custom_colors,
    custom_color_values = custom_color_values
  )
  subchunkify(
    consensus_plt, fig_width = 12, fig_height = 10,
    caption = sprintf(
      "'Consensus neighborhood heatmap, aggregated across all runs using %s and k = %s.'",
      clust_method_key, k_key
    )
  )
  
  cat("\n\n##### GCs per Cluster\n\n")
  bar_plt <- plot_gcs_per_cluster(
    cluster_ids = consensus_cluster_ids,
    gcs = metadata$train$GC_NAME
  )
  subchunkify(
    plotly::ggplotly(bar_plt),
    caption = sprintf(
      "'GC composition for each estimated consensus cluster using %s and k = %s.'",
      clust_method_key, k_key
    )
  )
  
  cat("\n\n#### Local Stability\n\n")
  cat("\n\n##### On Galactic Coordinates\n\n")
  local_stability <- eval_local_stability(
    nbhd_mat, consensus_cluster_ids
  )
  local_stability_plt <- plot_local_stability_on_galactic(
    local_stability = local_stability,
    cluster_ids = consensus_cluster_ids,
    metadata = metadata$train
  )
  subchunkify(
    local_stability_plt, fig_width = 10, fig_height = 6,
    caption = "'Galactic coordinates of stars, colored by its local cluster stability.'"
  )
  # subchunkify(plotly::ggplotly(local_stability_plt))
  
  cat("\n\n##### On Dimension Reduction\n\n")
  dr_plt <- plot_local_stability_on_dr(
    local_stability = local_stability,
    cluster_ids = consensus_cluster_ids,
    metadata = metadata$train,
    dr_data = dr_plt_df
  )
  subchunkify(
    dr_plt, fig_width = 12, fig_height = 6,
    caption = "'Dimension reduction visualizations of stars, colored by its local cluster stability.'"
  )
  # subchunkify(plotly::ggplotly(dr_plt))
  
  cat("\n\n#### Abundance per Cluster\n\n")
  abundance_plt <- dplyr::bind_cols(
    cluster_id = as.factor(consensus_cluster_ids),
    data_mean_imputed$train
  ) |> 
    tidyr::pivot_longer(
      cols = -cluster_id,
      names_to = "Feature",
      values_to = "Abundance"
    ) |> 
    ggplot2::ggplot() +
    ggplot2::aes(x = Abundance, fill = cluster_id) +
    ggplot2::facet_wrap(~ Feature, scales = "free") +
    ggplot2::geom_density(alpha = 0.5) +
    ggplot2::labs(y = "Density", fill = "Cluster ID") +
    vthemes::theme_vmodern()
  subchunkify(
    plotly::ggplotly(abundance_plt),
    fig_width = 12, fig_height = 10,
    caption = "'Abundance of features per estimated consensus cluster.'"
  )
  
  cat("\n\n:::\n\n")
}
```

:::

## Aside: What ifs...

In this section, we explore two hypothetical "what if" scenarios purely for didactic purposes.

**First, what if we had chosen to use Spectral Clustering (n\_neighbors = 60) with $k = 9$ clusters, which yields a similar stability score as K-means with $k = 8$ clusters?**

In short, the clusters from Spectral Clustering (n\_neighbors = 60) with $k = 9$ clusters are quite similar to the that from K-means with $k = 8$ clusters. 
This is most easily seen in the neighborhood heatmap below, which shows the co-cluster membership of each pair of stars. 
Red indicates that the two stars were in the same cluster in both clustering methods. 
Black indicates that the two stars were in different clusters in both clustering methods. 
White indicates that the two stars were in the same cluster in one clustering method but not the other. 

Notably, cluster 2 from spectral clustering perfectly matches cluster 2 from K-means clustering. There is also very close alignment between several other clusters across methods, namely:

- Spectral cluster 1 and K-means cluster 1
- Spectral cluster 3 and K-means cluster 3
- Spectral cluster 7 and K-means cluster 6
- Spectral cluster 9 and K-means cluster 8

Furthermore, it appears that K-means cluster 4 is being split into two clusters by spectral clustering (clusters 4 and 6).

:::{.panel-tabset}

### Comparison Summary

```{r}
#| output: asis
#| echo: false

consensus_clusters_ls <- list(
  "k = 9: Spectral (n_neighbors = 60)" = consensus_out_ls$`k = 9: Spectral (n_neighbors = 60)`$cluster_ids,
  "k = 8: K-means" = consensus_out_ls$`k = 8: K-means`$cluster_ids
)

cat("\n\n#### Neighborhood Heatmap\n\n")
nbhd_mat <- get_consensus_neighborhood_matrix(consensus_clusters_ls)
heatmap_plt <- ggwrappers::plot_hclust_heatmap(
  X = as.data.frame(nbhd_mat),
  x_groups = paste0(
    "Spectral\nCluster ", 
    consensus_clusters_ls$`k = 9: Spectral (n_neighbors = 60)`
  ),
  y_groups = paste0(
    "K-means\nCluster ", 
    consensus_clusters_ls$`k = 8: K-means`
  ),
  show_ytext = FALSE,
  show_xtext = FALSE
) +
  ggplot2::scale_fill_gradientn(
    colors = custom_colors, values = scales::rescale(custom_color_values)
  ) +
  ggplot2::labs(
    fill = "Frequency", x = "Star", y = "Star"
  ) +
  ggplot2::theme(
    panel.spacing = grid::unit(.05, "lines"),
    panel.border = ggplot2::element_rect(
      color = "black", fill = NA, linewidth = 0
    )
  )
subchunkify(
  heatmap_plt, fig_width = 12, fig_height = 10,
  caption = "'Consensus neighborhood heatmap between consensus clusters using Spectral (n_neighbors = 60) with k = 9 and K-means with k = 8.'"
)

cat("\n\n#### GCs per Cluster\n\n")
aligned_consensus_cluster_ls <- purrr::map(
  consensus_clusters_ls,
  ~ align_clusters(consensus_clusters_ls[[1]], factor(.x, levels = 1:9))
)
plt_df <- aligned_consensus_cluster_ls |> 
  dplyr::bind_cols() |> 
  dplyr::mutate(
    GC_NAME = metadata$train$GC_NAME
  ) |> 
  tidyr::pivot_longer(
    cols = -c(GC_NAME),
    names_to = "Pipeline",
    values_to = "cluster_id"
  )
bar_plt <- plt_df |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = as.factor(cluster_id),
    fill = GC_NAME,
    label = Pipeline
  ) +
  ggplot2::geom_bar(position = "fill") +
  ggplot2::facet_wrap(~ Pipeline) +
  ggplot2::labs(x = "Cluster ID", y = "Proportion") +
  ggplot2::theme(
    axis.text.x = ggplot2::element_blank(),
    axis.ticks.x = ggplot2::element_blank()
  ) +
  vthemes::theme_vmodern()
subchunkify(
  plotly::ggplotly(bar_plt), fig_height = 10, fig_width = 12,
  caption = "'Comparison of GC composition for each estimated consensus cluster.'"
)
```

```{r}
#| output: asis
#| echo: false

for (key in c("k = 9: Spectral (n_neighbors = 60)")) {
  cat(sprintf("\n\n### %s\n\n", key))
  cat("\n\n:::{.panel-tabset .nav-pills}\n\n")
  clust_method_key <- stringr::str_remove(key, ".*:\\s")
  k_key <- stringr::str_extract(key, "(?<=k = )\\d+")
  nbhd_mat <- nbhd_mat_ls[[key]]
  consensus_cluster_ids <- consensus_out_ls[[key]]$cluster_ids
  
  cat("\n\n#### Overview\n\n")
  cat("\n\n##### Consensus Neighborhood Heatmap\n\n")
  consensus_plt <- plot_consensus_clusters(
    nbhd_mat,
    leaf_colors = as.factor(consensus_cluster_ids),
    custom_colors = custom_colors,
    custom_color_values = custom_color_values
  )
  subchunkify(
    consensus_plt, fig_width = 12, fig_height = 10,
    caption = sprintf(
      "'Consensus neighborhood heatmap, aggregated across all runs using %s and k = %s.'",
      clust_method_key, k_key
    )
  )
  
  cat("\n\n##### GCs per Cluster\n\n")
  bar_plt <- plot_gcs_per_cluster(
    cluster_ids = consensus_cluster_ids,
    gcs = metadata$train$GC_NAME
  )
  subchunkify(
    plotly::ggplotly(bar_plt),
    caption = "'GC composition for each estimated consensus cluster.'"
  )
  
  cat("\n\n#### Local Stability\n\n")
  cat("\n\n##### On Galactic Coordinates\n\n")
  local_stability <- eval_local_stability(
    nbhd_mat, consensus_cluster_ids
  )
  local_stability_plt <- plot_local_stability_on_galactic(
    local_stability = local_stability,
    cluster_ids = consensus_cluster_ids,
    metadata = metadata$train
  )
  subchunkify(
    local_stability_plt, fig_width = 10, fig_height = 6,
    caption = "'Galactic coordinates of stars, colored by its local cluster stability.'"
  )
  # subchunkify(plotly::ggplotly(local_stability_plt))
  
  cat("\n\n##### On Dimension Reduction\n\n")
  dr_plt <- plot_local_stability_on_dr(
    local_stability = local_stability,
    cluster_ids = consensus_cluster_ids,
    metadata = metadata$train,
    dr_data = dr_plt_df
  )
  subchunkify(
    dr_plt, fig_width = 12, fig_height = 6,
    caption = "'Dimension reduction visualizations of stars, colored by its local cluster stability.'"
  )
  # subchunkify(plotly::ggplotly(dr_plt))
  
  cat("\n\n:::\n\n")
}
```

:::

**Second, what if we had chosen to use only the most stable data preprocessing + clustering pipeline (i.e., K-means [Mean-imputed + Small + Raw]) instead of using the consensus across all data preprocessing pipelines?**

The most stable data preprocessing + clustering pipeline choice is K-means, applied to the small feature set with mean-imputation and no dimension reduction prior to clustering. 
Applying this particular instantiation of K-means led to reasonable clusters, shown below. 
However, in the neighborhood heatmap, we observe an important difference between the consensus K-means pipeline (aggregated across all data preprocessing pipelines) versus the most stable K-means pipeline (using only the most stable data preprocessing choice) --- that is, the most stable K-means pipeline grouped the consensus cluster 1 and consensus cluster 2 into a single cluster and created what appears to be a small and seemingly noisy cluster ("Most Stable Cluster 6"). 
Examining the GCs in each estimated cluster, we further see that the consensus K-means pipeline was able to separate two important GCs into distinct clusters --- NGC6121 (dark green bar in cluster 1) and NGC0104 (orange bar in cluster 2) --- while the most stable K-means pipeline grouped these two GCs into a single cluster.
This is to show that the diversity of clusters across data preprocessing pipelines can help to improve the quality of the resulting clusters.

```{r}
#| echo: false

best_clust_pipes <- list(
  "k = 8: K-means" = list(
    clust_method_name = "K-means", 
    pipeline_name = "K-means [Mean-imputed + Small + Raw]", 
    k = 8
  )
)

consensus_clusters_results_path <- file.path(
  RESULTS_PATH, "consensus_clusters_train_most_stable_only.rds"
)
consensus_nbhd_results_path <- file.path(
  RESULTS_PATH, "consensus_neighborhood_matrices_train_most_stable_only.rds"
)
if (!file.exists(consensus_clusters_results_path) | 
    !file.exists(consensus_nbhd_results_path)) {
  pipe_nbhd_mat_ls <- list()
  pipe_consensus_out_ls <- list()
  for (i in 1:length(best_clust_pipes)) {
    clust_method_name <- best_clust_pipes[[i]]$clust_method_name
    pipeline_name <- best_clust_pipes[[i]]$pipeline_name
    k <- best_clust_pipes[[i]]$k
    key <- names(best_clust_pipes)[i]
    
    stable_clusters_tib_all <- tibble::tibble(
      name = names(clust_subsampled_fit_ls),
      cluster_ids = clust_subsampled_fit_ls
    ) |> 
      annotate_clustering_results() |> 
      dplyr::filter(
        k == !!k,
        clust_method_name == !!clust_method_name,
        pipeline_name == !!pipeline_name
      )
  
    keep_clust_ls <- stable_clusters_tib_all$cluster_ids
    # compute neighborhood matrix
    pipe_nbhd_mat_ls[[key]] <- get_consensus_neighborhood_matrix(keep_clust_ls)
    # aggregate stable clusters using consensus clustering
    pipe_consensus_out_ls[[key]] <- fit_consensus_clusters(
      pipe_nbhd_mat_ls[[key]], k = k
    )
  }
  saveRDS(pipe_consensus_out_ls, file = consensus_clusters_results_path)
  saveRDS(pipe_nbhd_mat_ls, file = consensus_nbhd_results_path)
} else {
  # read in fitted clustering pipelines (if already cached)
  pipe_consensus_out_ls <- readRDS(consensus_clusters_results_path)
  pipe_nbhd_mat_ls <- readRDS(consensus_nbhd_results_path)
}
```

:::{.panel-tabset}

### Comparison Summary

```{r}
#| output: asis
#| echo: false

consensus_clusters_ls <- list(
  "k = 8: K-means [All]" = consensus_out_ls$`k = 8: K-means`$cluster_ids,
  "k = 8: K-means [Most Stable]" = pipe_consensus_out_ls$`k = 8: K-means`$cluster_ids
)

cat("\n\n#### Neighborhood Heatmap\n\n")
nbhd_mat <- get_consensus_neighborhood_matrix(consensus_clusters_ls)
heatmap_plt <- ggwrappers::plot_hclust_heatmap(
  X = as.data.frame(nbhd_mat),
  x_groups = paste0(
    "Consensus\nCluster ", 
    consensus_clusters_ls$`k = 8: K-means [All]`
  ),
  y_groups = paste0(
    "Most Stable Only\nCluster ", 
    consensus_clusters_ls$`k = 8: K-means [Most Stable]`
  ),
  show_ytext = FALSE,
  show_xtext = FALSE
) +
  ggplot2::scale_fill_gradientn(
    colors = custom_colors, values = scales::rescale(custom_color_values)
  ) +
  ggplot2::labs(
    fill = "Frequency", x = "Star", y = "Star"
  ) +
  ggplot2::theme(
    panel.spacing = grid::unit(.05, "lines"),
    panel.border = ggplot2::element_rect(
      color = "black", fill = NA, linewidth = 0
    )
  )
subchunkify(
  heatmap_plt, fig_width = 12, fig_height = 10,
  caption = "'Consensus neighborhood heatmap between consensus clusters using K-means with k = 8, aggregated across all data preprocessing pipelines versus using only the most stable clustering pipeline.'"
)

cat("\n\n#### GCs per Cluster\n\n")
aligned_consensus_cluster_ls <- purrr::map(
  consensus_clusters_ls,
  ~ align_clusters(consensus_clusters_ls[[1]], factor(.x, levels = 1:9))
)
plt_df <- aligned_consensus_cluster_ls |> 
  dplyr::bind_cols() |> 
  dplyr::mutate(
    GC_NAME = metadata$train$GC_NAME
  ) |> 
  tidyr::pivot_longer(
    cols = -c(GC_NAME),
    names_to = "Pipeline",
    values_to = "cluster_id"
  )
bar_plt <- plt_df |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    x = as.factor(cluster_id),
    fill = GC_NAME,
    label = Pipeline
  ) +
  ggplot2::geom_bar(position = "fill") +
  ggplot2::facet_wrap(~ Pipeline) +
  ggplot2::labs(x = "Cluster ID", y = "Proportion") +
  ggplot2::theme(
    axis.text.x = ggplot2::element_blank(),
    axis.ticks.x = ggplot2::element_blank()
  ) +
  vthemes::theme_vmodern()
subchunkify(
  plotly::ggplotly(bar_plt), fig_height = 10, fig_width = 12,
  caption = "'Comparison of GC composition for each estimated consensus cluster.'"
)
```


```{r}
#| output: asis
#| echo: false

for (key in names(best_clust_pipes)) {
  cat(sprintf("\n\n### %s\n\n", key))
  cat("\n\n:::{.panel-tabset .nav-pills}\n\n")
  nbhd_mat <- pipe_nbhd_mat_ls[[key]]
  consensus_cluster_ids <- pipe_consensus_out_ls[[key]]$cluster_ids
  
  cat("\n\n#### Overview\n\n")
  cat("\n\n##### Consensus Neighborhood Heatmap\n\n")
  consensus_plt <- plot_consensus_clusters(
    nbhd_mat,
    leaf_colors = as.factor(consensus_cluster_ids),
    custom_colors = custom_colors,
    custom_color_values = custom_color_values
  )
  subchunkify(
    consensus_plt, fig_width = 12, fig_height = 10,
    caption = sprintf(
      "'Consensus neighborhood heatmap, aggregated across all subsample runs using %s'",
      key
    )
  )
  
  cat("\n\n##### GCs per Cluster\n\n")
  bar_plt <- plot_gcs_per_cluster(
    cluster_ids = consensus_cluster_ids,
    gcs = metadata$train$GC_NAME
  )
  subchunkify(
    plotly::ggplotly(bar_plt),
    caption = "'GC composition for each estimated consensus cluster.'"
  )
  
  cat("\n\n#### Local Stability\n\n")
  cat("\n\n##### On Galactic Coordinates\n\n")
  local_stability <- eval_local_stability(
    nbhd_mat, consensus_cluster_ids
  )
  local_stability_plt <- plot_local_stability_on_galactic(
    local_stability = local_stability,
    cluster_ids = consensus_cluster_ids,
    metadata = metadata$train
  )
  subchunkify(
    local_stability_plt, fig_width = 10, fig_height = 6,
    caption = "'Galactic coordinates of stars, colored by its local cluster stability.'"
  )
  # subchunkify(plotly::ggplotly(local_stability_plt))
  
  cat("\n\n##### On Dimension Reduction\n\n")
  dr_plt <- plot_local_stability_on_dr(
    local_stability = local_stability,
    cluster_ids = consensus_cluster_ids,
    metadata = metadata$train,
    dr_data = dr_plt_df
  )
  subchunkify(
    dr_plt, fig_width = 12, fig_height = 6,
    caption = "'Dimension reduction visualizations of stars, colored by its local cluster stability.'"
  )
  # subchunkify(plotly::ggplotly(dr_plt))
  cat("\n\n:::\n\n")
}
```

:::


